{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5794505e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T11:17:22.448413Z",
     "iopub.status.busy": "2024-01-06T11:17:22.448052Z",
     "iopub.status.idle": "2024-01-06T11:19:05.060911Z",
     "shell.execute_reply": "2024-01-06T11:19:05.059939Z"
    },
    "papermill": {
     "duration": 102.62358,
     "end_time": "2024-01-06T11:19:05.062988",
     "exception": false,
     "start_time": "2024-01-06T11:17:22.439408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading non-defended dataset for closed-world scenario\n",
      "Dataset directory: /kaggle/input/dataset-non-defended-close-world/\n",
      "X_train loaded\n",
      "y_train loaded\n",
      "X_valid loaded\n",
      "y_valid loaded\n",
      "X_test loaded\n",
      "y_test loaded\n",
      "Data dimensions:\n",
      "X: Training data's shape :  (76000, 5000)\n",
      "y: Training data's shape :  (76000,)\n",
      "X: Validation data's shape :  (9500, 5000)\n",
      "y: Validation data's shape :  (9500,)\n",
      "X: Testing data's shape :  (9500, 5000)\n",
      "y: Testing data's shape :  (9500,)\n",
      "Merged data dimensions:\n",
      "X: Merged data's shape :  (95000, 5000)\n",
      "y: Merged data's shape :  (95000,)\n",
      "Features of the merged dataset:\n",
      "[[ 1. -1. -1. ... -1. -1. -1.]\n",
      " [-1.  1.  1. ...  0.  0.  0.]\n",
      " [ 1. -1. -1. ...  0.  0.  0.]\n",
      " ...\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [ 1. -1.  1. ...  0.  0.  0.]]\n",
      "Class distribution:\n",
      "Class 0: 1000 samples\n",
      "Class 1: 1000 samples\n",
      "Class 2: 1000 samples\n",
      "Class 3: 1000 samples\n",
      "Class 4: 1000 samples\n",
      "Class 5: 1000 samples\n",
      "Class 6: 1000 samples\n",
      "Class 7: 1000 samples\n",
      "Class 8: 1000 samples\n",
      "Class 9: 1000 samples\n",
      "Class 10: 1000 samples\n",
      "Class 11: 1000 samples\n",
      "Class 12: 1000 samples\n",
      "Class 13: 1000 samples\n",
      "Class 14: 1000 samples\n",
      "Class 15: 1000 samples\n",
      "Class 16: 1000 samples\n",
      "Class 17: 1000 samples\n",
      "Class 18: 1000 samples\n",
      "Class 19: 1000 samples\n",
      "Class 20: 1000 samples\n",
      "Class 21: 1000 samples\n",
      "Class 22: 1000 samples\n",
      "Class 23: 1000 samples\n",
      "Class 24: 1000 samples\n",
      "Class 25: 1000 samples\n",
      "Class 26: 1000 samples\n",
      "Class 27: 1000 samples\n",
      "Class 28: 1000 samples\n",
      "Class 29: 1000 samples\n",
      "Class 30: 1000 samples\n",
      "Class 31: 1000 samples\n",
      "Class 32: 1000 samples\n",
      "Class 33: 1000 samples\n",
      "Class 34: 1000 samples\n",
      "Class 35: 1000 samples\n",
      "Class 36: 1000 samples\n",
      "Class 37: 1000 samples\n",
      "Class 38: 1000 samples\n",
      "Class 39: 1000 samples\n",
      "Class 40: 1000 samples\n",
      "Class 41: 1000 samples\n",
      "Class 42: 1000 samples\n",
      "Class 43: 1000 samples\n",
      "Class 44: 1000 samples\n",
      "Class 45: 1000 samples\n",
      "Class 46: 1000 samples\n",
      "Class 47: 1000 samples\n",
      "Class 48: 1000 samples\n",
      "Class 49: 1000 samples\n",
      "Class 50: 1000 samples\n",
      "Class 51: 1000 samples\n",
      "Class 52: 1000 samples\n",
      "Class 53: 1000 samples\n",
      "Class 54: 1000 samples\n",
      "Class 55: 1000 samples\n",
      "Class 56: 1000 samples\n",
      "Class 57: 1000 samples\n",
      "Class 58: 1000 samples\n",
      "Class 59: 1000 samples\n",
      "Class 60: 1000 samples\n",
      "Class 61: 1000 samples\n",
      "Class 62: 1000 samples\n",
      "Class 63: 1000 samples\n",
      "Class 64: 1000 samples\n",
      "Class 65: 1000 samples\n",
      "Class 66: 1000 samples\n",
      "Class 67: 1000 samples\n",
      "Class 68: 1000 samples\n",
      "Class 69: 1000 samples\n",
      "Class 70: 1000 samples\n",
      "Class 71: 1000 samples\n",
      "Class 72: 1000 samples\n",
      "Class 73: 1000 samples\n",
      "Class 74: 1000 samples\n",
      "Class 75: 1000 samples\n",
      "Class 76: 1000 samples\n",
      "Class 77: 1000 samples\n",
      "Class 78: 1000 samples\n",
      "Class 79: 1000 samples\n",
      "Class 80: 1000 samples\n",
      "Class 81: 1000 samples\n",
      "Class 82: 1000 samples\n",
      "Class 83: 1000 samples\n",
      "Class 84: 1000 samples\n",
      "Class 85: 1000 samples\n",
      "Class 86: 1000 samples\n",
      "Class 87: 1000 samples\n",
      "Class 88: 1000 samples\n",
      "Class 89: 1000 samples\n",
      "Class 90: 1000 samples\n",
      "Class 91: 1000 samples\n",
      "Class 92: 1000 samples\n",
      "Class 93: 1000 samples\n",
      "Class 94: 1000 samples\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout, ELU\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from keras import layers, models\n",
    "\n",
    "# ... (your existing code)\n",
    "def LoadDataNoDefCW():\n",
    "\n",
    "    print(\"Loading non-defended dataset for closed-world scenario\")\n",
    "\n",
    "    # Point to the directory storing data\n",
    "    dataset_dir = '/kaggle/input/dataset-non-defended-close-world/'\n",
    "\n",
    "    # Debug: Print dataset directory\n",
    "    print(\"Dataset directory:\", dataset_dir)\n",
    "\n",
    "    try:\n",
    "        # Load training data\n",
    "        with open(dataset_dir + 'X_train_NoDef.pkl', 'rb') as handle:\n",
    "            X_train = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"X_train loaded\")\n",
    "\n",
    "        with open(dataset_dir + 'y_train_NoDef.pkl', 'rb') as handle:\n",
    "            y_train = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"y_train loaded\")\n",
    "\n",
    "        # Load validation data\n",
    "        with open(dataset_dir + 'X_valid_NoDef.pkl', 'rb') as handle:\n",
    "            X_valid = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"X_valid loaded\")\n",
    "\n",
    "        with open(dataset_dir + 'y_valid_NoDef.pkl', 'rb') as handle:\n",
    "            y_valid = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"y_valid loaded\")\n",
    "\n",
    "        # Load testing data\n",
    "        with open(dataset_dir + 'X_test_NoDef.pkl', 'rb') as handle:\n",
    "            X_test = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"X_test loaded\")\n",
    "\n",
    "        with open(dataset_dir + 'y_test_NoDef.pkl', 'rb') as handle:\n",
    "            y_test = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"y_test loaded\")\n",
    "\n",
    "        print(\"Data dimensions:\")\n",
    "        print(\"X: Training data's shape : \", X_train.shape)\n",
    "        print(\"y: Training data's shape : \", y_train.shape)\n",
    "        print(\"X: Validation data's shape : \", X_valid.shape)\n",
    "        print(\"y: Validation data's shape : \", y_valid.shape)\n",
    "        print(\"X: Testing data's shape : \", X_test.shape)\n",
    "        print(\"y: Testing data's shape : \", y_test.shape)\n",
    "\n",
    "        # Merge datasets\n",
    "        X_all = np.concatenate((X_train, X_valid, X_test), axis=0)\n",
    "        y_all = np.concatenate((y_train, y_valid, y_test), axis=0)\n",
    "\n",
    "        print(\"Merged data dimensions:\")\n",
    "        print(\"X: Merged data's shape : \", X_all.shape)\n",
    "        print(\"y: Merged data's shape : \", y_all.shape)\n",
    "        \n",
    "        # Print features of the merged dataset\n",
    "        print(\"Features of the merged dataset:\")\n",
    "        print(X_all)\n",
    "        \n",
    "        # Check if the class distribution is balanced\n",
    "        unique_classes, class_counts = np.unique(y_all, return_counts=True)\n",
    "        class_distribution = dict(zip(unique_classes, class_counts))\n",
    "\n",
    "        print(\"Class distribution:\")\n",
    "        for class_label, count in class_distribution.items():\n",
    "            print(f\"Class {class_label}: {count} samples\")\n",
    "\n",
    "        # Plot the class distribution\n",
    "        plt.bar(class_distribution.keys(), class_distribution.values())\n",
    "        plt.xlabel('Class Label')\n",
    "        plt.ylabel('Number of Samples')\n",
    "        plt.title('Class Distribution')\n",
    "        plt.show()\n",
    "        \n",
    "        return X_all, y_all\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        return None\n",
    "\n",
    "# Call the function to load and merge data\n",
    "X_all, y_all = LoadDataNoDefCW()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d375efb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T11:19:05.077745Z",
     "iopub.status.busy": "2024-01-06T11:19:05.077216Z",
     "iopub.status.idle": "2024-01-06T12:29:55.942705Z",
     "shell.execute_reply": "2024-01-06T12:29:55.941739Z"
    },
    "papermill": {
     "duration": 4250.895231,
     "end_time": "2024-01-06T12:29:55.964936",
     "exception": false,
     "start_time": "2024-01-06T11:19:05.069705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built successfully.\n",
      "Model Summary:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv1D)       (None, 5000, 32)          288       \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 5000, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block1_adv_act1 (ELU)       (None, 5000, 32)          0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv1D)       (None, 5000, 32)          8224      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 5000, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block1_adv_act2 (ELU)       (None, 5000, 32)          0         \n",
      "                                                                 \n",
      " block1_pool (MaxPooling1D)  (None, 1250, 32)          0         \n",
      "                                                                 \n",
      " block1_dropout (Dropout)    (None, 1250, 32)          0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv1D)       (None, 1250, 64)          16448     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 1250, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block2_adv_act1 (ELU)       (None, 1250, 64)          0         \n",
      "                                                                 \n",
      " block2_conv2 (Conv1D)       (None, 1250, 64)          32832     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 1250, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block2_adv_act2 (ELU)       (None, 1250, 64)          0         \n",
      "                                                                 \n",
      " block2_pool (MaxPooling1D)  (None, 313, 64)           0         \n",
      "                                                                 \n",
      " block2_dropout (Dropout)    (None, 313, 64)           0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv1D)       (None, 313, 128)          65664     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 313, 128)          512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block3_adv_act1 (ELU)       (None, 313, 128)          0         \n",
      "                                                                 \n",
      " block3_conv2 (Conv1D)       (None, 313, 128)          131200    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 313, 128)          512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block3_adv_act2 (ELU)       (None, 313, 128)          0         \n",
      "                                                                 \n",
      " block3_pool (MaxPooling1D)  (None, 79, 128)           0         \n",
      "                                                                 \n",
      " block3_dropout (Dropout)    (None, 79, 128)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10112)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 512)               5177856   \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " fc1_act (Activation)        (None, 512)               0         \n",
      "                                                                 \n",
      " fc1_dropout (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " fc2_act (Activation)        (None, 512)               0         \n",
      "                                                                 \n",
      " fc2_dropout (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " fc3 (Dense)                 (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " fc3_act (Activation)        (None, 512)               0         \n",
      "                                                                 \n",
      " fc3_dropout (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " fc_final (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5970890 (22.78 MB)\n",
      "Trainable params: 5966922 (22.76 MB)\n",
      "Non-trainable params: 3968 (15.50 KB)\n",
      "_________________________________________________________________\n",
      "Loading non-defended dataset for closed-world scenario\n",
      "Dataset directory: /kaggle/input/dataset-non-defended-close-world/\n",
      "X_train loaded\n",
      "y_train loaded\n",
      "X_valid loaded\n",
      "y_valid loaded\n",
      "X_test loaded\n",
      "y_test loaded\n",
      "Data dimensions:\n",
      "X: Training data's shape :  (76000, 5000)\n",
      "y: Training data's shape :  (76000,)\n",
      "X: Validation data's shape :  (9500, 5000)\n",
      "y: Validation data's shape :  (9500,)\n",
      "X: Testing data's shape :  (9500, 5000)\n",
      "y: Testing data's shape :  (9500,)\n",
      "Merged data dimensions:\n",
      "X: Merged data's shape :  (95000, 5000)\n",
      "y: Merged data's shape :  (95000,)\n",
      "Features of the merged dataset:\n",
      "[[ 1. -1. -1. ... -1. -1. -1.]\n",
      " [-1.  1.  1. ...  0.  0.  0.]\n",
      " [ 1. -1. -1. ...  0.  0.  0.]\n",
      " ...\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [ 1. -1.  1. ...  0.  0.  0.]]\n",
      "Class distribution:\n",
      "Class 0: 1000 samples\n",
      "Class 1: 1000 samples\n",
      "Class 2: 1000 samples\n",
      "Class 3: 1000 samples\n",
      "Class 4: 1000 samples\n",
      "Class 5: 1000 samples\n",
      "Class 6: 1000 samples\n",
      "Class 7: 1000 samples\n",
      "Class 8: 1000 samples\n",
      "Class 9: 1000 samples\n",
      "Class 10: 1000 samples\n",
      "Class 11: 1000 samples\n",
      "Class 12: 1000 samples\n",
      "Class 13: 1000 samples\n",
      "Class 14: 1000 samples\n",
      "Class 15: 1000 samples\n",
      "Class 16: 1000 samples\n",
      "Class 17: 1000 samples\n",
      "Class 18: 1000 samples\n",
      "Class 19: 1000 samples\n",
      "Class 20: 1000 samples\n",
      "Class 21: 1000 samples\n",
      "Class 22: 1000 samples\n",
      "Class 23: 1000 samples\n",
      "Class 24: 1000 samples\n",
      "Class 25: 1000 samples\n",
      "Class 26: 1000 samples\n",
      "Class 27: 1000 samples\n",
      "Class 28: 1000 samples\n",
      "Class 29: 1000 samples\n",
      "Class 30: 1000 samples\n",
      "Class 31: 1000 samples\n",
      "Class 32: 1000 samples\n",
      "Class 33: 1000 samples\n",
      "Class 34: 1000 samples\n",
      "Class 35: 1000 samples\n",
      "Class 36: 1000 samples\n",
      "Class 37: 1000 samples\n",
      "Class 38: 1000 samples\n",
      "Class 39: 1000 samples\n",
      "Class 40: 1000 samples\n",
      "Class 41: 1000 samples\n",
      "Class 42: 1000 samples\n",
      "Class 43: 1000 samples\n",
      "Class 44: 1000 samples\n",
      "Class 45: 1000 samples\n",
      "Class 46: 1000 samples\n",
      "Class 47: 1000 samples\n",
      "Class 48: 1000 samples\n",
      "Class 49: 1000 samples\n",
      "Class 50: 1000 samples\n",
      "Class 51: 1000 samples\n",
      "Class 52: 1000 samples\n",
      "Class 53: 1000 samples\n",
      "Class 54: 1000 samples\n",
      "Class 55: 1000 samples\n",
      "Class 56: 1000 samples\n",
      "Class 57: 1000 samples\n",
      "Class 58: 1000 samples\n",
      "Class 59: 1000 samples\n",
      "Class 60: 1000 samples\n",
      "Class 61: 1000 samples\n",
      "Class 62: 1000 samples\n",
      "Class 63: 1000 samples\n",
      "Class 64: 1000 samples\n",
      "Class 65: 1000 samples\n",
      "Class 66: 1000 samples\n",
      "Class 67: 1000 samples\n",
      "Class 68: 1000 samples\n",
      "Class 69: 1000 samples\n",
      "Class 70: 1000 samples\n",
      "Class 71: 1000 samples\n",
      "Class 72: 1000 samples\n",
      "Class 73: 1000 samples\n",
      "Class 74: 1000 samples\n",
      "Class 75: 1000 samples\n",
      "Class 76: 1000 samples\n",
      "Class 77: 1000 samples\n",
      "Class 78: 1000 samples\n",
      "Class 79: 1000 samples\n",
      "Class 80: 1000 samples\n",
      "Class 81: 1000 samples\n",
      "Class 82: 1000 samples\n",
      "Class 83: 1000 samples\n",
      "Class 84: 1000 samples\n",
      "Class 85: 1000 samples\n",
      "Class 86: 1000 samples\n",
      "Class 87: 1000 samples\n",
      "Class 88: 1000 samples\n",
      "Class 89: 1000 samples\n",
      "Class 90: 1000 samples\n",
      "Class 91: 1000 samples\n",
      "Class 92: 1000 samples\n",
      "Class 93: 1000 samples\n",
      "Class 94: 1000 samples\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on Fold 1/5\n",
      "Model built successfully.\n",
      "Epoch 1/10\n",
      "1188/1188 - 90s - loss: 1.4830 - accuracy: 0.6198 - val_loss: 0.3482 - val_accuracy: 0.9152 - 90s/epoch - 76ms/step\n",
      "Epoch 2/10\n",
      "1188/1188 - 80s - loss: 0.5251 - accuracy: 0.8635 - val_loss: 0.2221 - val_accuracy: 0.9452 - 80s/epoch - 67ms/step\n",
      "Epoch 3/10\n",
      "1188/1188 - 80s - loss: 0.3790 - accuracy: 0.9010 - val_loss: 0.2006 - val_accuracy: 0.9491 - 80s/epoch - 67ms/step\n",
      "Epoch 4/10\n",
      "1188/1188 - 79s - loss: 0.3087 - accuracy: 0.9191 - val_loss: 0.1705 - val_accuracy: 0.9581 - 79s/epoch - 67ms/step\n",
      "Epoch 5/10\n",
      "1188/1188 - 80s - loss: 0.2667 - accuracy: 0.9296 - val_loss: 0.1477 - val_accuracy: 0.9636 - 80s/epoch - 68ms/step\n",
      "Epoch 6/10\n",
      "1188/1188 - 80s - loss: 0.2283 - accuracy: 0.9393 - val_loss: 0.1352 - val_accuracy: 0.9687 - 80s/epoch - 68ms/step\n",
      "Epoch 7/10\n",
      "1188/1188 - 81s - loss: 0.2048 - accuracy: 0.9451 - val_loss: 0.1244 - val_accuracy: 0.9702 - 81s/epoch - 68ms/step\n",
      "Epoch 8/10\n",
      "1188/1188 - 81s - loss: 0.1896 - accuracy: 0.9491 - val_loss: 0.1186 - val_accuracy: 0.9711 - 81s/epoch - 68ms/step\n",
      "Epoch 9/10\n",
      "1188/1188 - 81s - loss: 0.1761 - accuracy: 0.9529 - val_loss: 0.1277 - val_accuracy: 0.9694 - 81s/epoch - 68ms/step\n",
      "Epoch 10/10\n",
      "1188/1188 - 81s - loss: 0.1610 - accuracy: 0.9566 - val_loss: 0.1159 - val_accuracy: 0.9725 - 81s/epoch - 68ms/step\n",
      "594/594 - 6s - loss: 0.1159 - accuracy: 0.9725 - 6s/epoch - 10ms/step\n",
      "\n",
      "Fold 1 - Test Accuracy: 97.25%\n",
      "\n",
      "Training on Fold 2/5\n",
      "Model built successfully.\n",
      "Epoch 1/10\n",
      "1188/1188 - 90s - loss: 1.3783 - accuracy: 0.6441 - val_loss: 0.3370 - val_accuracy: 0.9120 - 90s/epoch - 76ms/step\n",
      "Epoch 2/10\n",
      "1188/1188 - 80s - loss: 0.5014 - accuracy: 0.8704 - val_loss: 0.2723 - val_accuracy: 0.9285 - 80s/epoch - 68ms/step\n",
      "Epoch 3/10\n",
      "1188/1188 - 80s - loss: 0.3692 - accuracy: 0.9038 - val_loss: 0.1755 - val_accuracy: 0.9562 - 80s/epoch - 68ms/step\n",
      "Epoch 4/10\n",
      "1188/1188 - 80s - loss: 0.3003 - accuracy: 0.9218 - val_loss: 0.1607 - val_accuracy: 0.9602 - 80s/epoch - 68ms/step\n",
      "Epoch 5/10\n",
      "1188/1188 - 80s - loss: 0.2579 - accuracy: 0.9321 - val_loss: 0.1453 - val_accuracy: 0.9639 - 80s/epoch - 68ms/step\n",
      "Epoch 6/10\n",
      "1188/1188 - 80s - loss: 0.2267 - accuracy: 0.9400 - val_loss: 0.1442 - val_accuracy: 0.9645 - 80s/epoch - 68ms/step\n",
      "Epoch 7/10\n",
      "1188/1188 - 80s - loss: 0.2065 - accuracy: 0.9451 - val_loss: 0.1406 - val_accuracy: 0.9642 - 80s/epoch - 68ms/step\n",
      "Epoch 8/10\n",
      "1188/1188 - 80s - loss: 0.1855 - accuracy: 0.9511 - val_loss: 0.1233 - val_accuracy: 0.9695 - 80s/epoch - 68ms/step\n",
      "Epoch 9/10\n",
      "1188/1188 - 80s - loss: 0.1689 - accuracy: 0.9553 - val_loss: 0.1234 - val_accuracy: 0.9701 - 80s/epoch - 68ms/step\n",
      "Epoch 10/10\n",
      "1188/1188 - 80s - loss: 0.1595 - accuracy: 0.9573 - val_loss: 0.1104 - val_accuracy: 0.9729 - 80s/epoch - 68ms/step\n",
      "594/594 - 6s - loss: 0.1104 - accuracy: 0.9729 - 6s/epoch - 10ms/step\n",
      "\n",
      "Fold 2 - Test Accuracy: 97.29%\n",
      "\n",
      "Training on Fold 3/5\n",
      "Model built successfully.\n",
      "Epoch 1/10\n",
      "1188/1188 - 90s - loss: 1.3880 - accuracy: 0.6438 - val_loss: 0.3882 - val_accuracy: 0.9032 - 90s/epoch - 76ms/step\n",
      "Epoch 2/10\n",
      "1188/1188 - 80s - loss: 0.4969 - accuracy: 0.8716 - val_loss: 0.2214 - val_accuracy: 0.9465 - 80s/epoch - 68ms/step\n",
      "Epoch 3/10\n",
      "1188/1188 - 80s - loss: 0.3574 - accuracy: 0.9073 - val_loss: 0.1854 - val_accuracy: 0.9535 - 80s/epoch - 68ms/step\n",
      "Epoch 4/10\n",
      "1188/1188 - 80s - loss: 0.3004 - accuracy: 0.9206 - val_loss: 0.1611 - val_accuracy: 0.9585 - 80s/epoch - 68ms/step\n",
      "Epoch 5/10\n",
      "1188/1188 - 80s - loss: 0.2551 - accuracy: 0.9321 - val_loss: 0.1518 - val_accuracy: 0.9639 - 80s/epoch - 68ms/step\n",
      "Epoch 6/10\n",
      "1188/1188 - 80s - loss: 0.2267 - accuracy: 0.9402 - val_loss: 0.1394 - val_accuracy: 0.9647 - 80s/epoch - 68ms/step\n",
      "Epoch 7/10\n",
      "1188/1188 - 80s - loss: 0.2043 - accuracy: 0.9462 - val_loss: 0.1385 - val_accuracy: 0.9669 - 80s/epoch - 68ms/step\n",
      "Epoch 8/10\n",
      "1188/1188 - 80s - loss: 0.1852 - accuracy: 0.9507 - val_loss: 0.1247 - val_accuracy: 0.9687 - 80s/epoch - 68ms/step\n",
      "Epoch 9/10\n",
      "1188/1188 - 80s - loss: 0.1689 - accuracy: 0.9554 - val_loss: 0.1152 - val_accuracy: 0.9723 - 80s/epoch - 68ms/step\n",
      "Epoch 10/10\n",
      "1188/1188 - 80s - loss: 0.1608 - accuracy: 0.9565 - val_loss: 0.1119 - val_accuracy: 0.9728 - 80s/epoch - 68ms/step\n",
      "594/594 - 6s - loss: 0.1119 - accuracy: 0.9728 - 6s/epoch - 10ms/step\n",
      "\n",
      "Fold 3 - Test Accuracy: 97.28%\n",
      "\n",
      "Training on Fold 4/5\n",
      "Model built successfully.\n",
      "Epoch 1/10\n",
      "1188/1188 - 90s - loss: 1.3964 - accuracy: 0.6404 - val_loss: 0.3389 - val_accuracy: 0.9144 - 90s/epoch - 75ms/step\n",
      "Epoch 2/10\n",
      "1188/1188 - 80s - loss: 0.5029 - accuracy: 0.8679 - val_loss: 0.2166 - val_accuracy: 0.9453 - 80s/epoch - 68ms/step\n",
      "Epoch 3/10\n",
      "1188/1188 - 80s - loss: 0.3600 - accuracy: 0.9070 - val_loss: 0.1759 - val_accuracy: 0.9551 - 80s/epoch - 68ms/step\n",
      "Epoch 4/10\n",
      "1188/1188 - 80s - loss: 0.3006 - accuracy: 0.9216 - val_loss: 0.1553 - val_accuracy: 0.9601 - 80s/epoch - 68ms/step\n",
      "Epoch 5/10\n",
      "1188/1188 - 80s - loss: 0.2541 - accuracy: 0.9326 - val_loss: 0.1483 - val_accuracy: 0.9619 - 80s/epoch - 68ms/step\n",
      "Epoch 6/10\n",
      "1188/1188 - 80s - loss: 0.2256 - accuracy: 0.9405 - val_loss: 0.1294 - val_accuracy: 0.9670 - 80s/epoch - 68ms/step\n",
      "Epoch 7/10\n",
      "1188/1188 - 80s - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1256 - val_accuracy: 0.9695 - 80s/epoch - 68ms/step\n",
      "Epoch 8/10\n",
      "1188/1188 - 80s - loss: 0.1848 - accuracy: 0.9508 - val_loss: 0.1211 - val_accuracy: 0.9701 - 80s/epoch - 68ms/step\n",
      "Epoch 9/10\n",
      "1188/1188 - 80s - loss: 0.1696 - accuracy: 0.9550 - val_loss: 0.1202 - val_accuracy: 0.9699 - 80s/epoch - 68ms/step\n",
      "Epoch 10/10\n",
      "1188/1188 - 80s - loss: 0.1589 - accuracy: 0.9582 - val_loss: 0.1053 - val_accuracy: 0.9733 - 80s/epoch - 68ms/step\n",
      "594/594 - 6s - loss: 0.1053 - accuracy: 0.9733 - 6s/epoch - 10ms/step\n",
      "\n",
      "Fold 4 - Test Accuracy: 97.33%\n",
      "\n",
      "Training on Fold 5/5\n",
      "Model built successfully.\n",
      "Epoch 1/10\n",
      "1188/1188 - 90s - loss: 1.4648 - accuracy: 0.6200 - val_loss: 0.3657 - val_accuracy: 0.9120 - 90s/epoch - 76ms/step\n",
      "Epoch 2/10\n",
      "1188/1188 - 80s - loss: 0.5263 - accuracy: 0.8631 - val_loss: 0.2709 - val_accuracy: 0.9299 - 80s/epoch - 68ms/step\n",
      "Epoch 3/10\n",
      "1188/1188 - 80s - loss: 0.3830 - accuracy: 0.9007 - val_loss: 0.1888 - val_accuracy: 0.9525 - 80s/epoch - 68ms/step\n",
      "Epoch 4/10\n",
      "1188/1188 - 80s - loss: 0.3065 - accuracy: 0.9201 - val_loss: 0.1583 - val_accuracy: 0.9606 - 80s/epoch - 68ms/step\n",
      "Epoch 5/10\n",
      "1188/1188 - 80s - loss: 0.2613 - accuracy: 0.9317 - val_loss: 0.1399 - val_accuracy: 0.9671 - 80s/epoch - 68ms/step\n",
      "Epoch 6/10\n",
      "1188/1188 - 80s - loss: 0.2352 - accuracy: 0.9374 - val_loss: 0.1357 - val_accuracy: 0.9654 - 80s/epoch - 68ms/step\n",
      "Epoch 7/10\n",
      "1188/1188 - 80s - loss: 0.2126 - accuracy: 0.9440 - val_loss: 0.1214 - val_accuracy: 0.9699 - 80s/epoch - 68ms/step\n",
      "Epoch 8/10\n",
      "1188/1188 - 80s - loss: 0.1913 - accuracy: 0.9498 - val_loss: 0.1150 - val_accuracy: 0.9729 - 80s/epoch - 68ms/step\n",
      "Epoch 9/10\n",
      "1188/1188 - 80s - loss: 0.1774 - accuracy: 0.9532 - val_loss: 0.1094 - val_accuracy: 0.9742 - 80s/epoch - 68ms/step\n",
      "Epoch 10/10\n",
      "1188/1188 - 80s - loss: 0.1649 - accuracy: 0.9566 - val_loss: 0.1054 - val_accuracy: 0.9749 - 80s/epoch - 68ms/step\n",
      "594/594 - 6s - loss: 0.1054 - accuracy: 0.9749 - 6s/epoch - 10ms/step\n",
      "\n",
      "Fold 5 - Test Accuracy: 97.49%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout, ELU\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DFNet:\n",
    "    @staticmethod\n",
    "    def build(input_shape, classes, dropout_rate1=0.7, dropout_rate2=0.5, dropout_rate_fc=0.5):\n",
    "        model = Sequential()\n",
    "\n",
    "        # ... (previous model definition remains unchanged)\n",
    "        # Block 1\n",
    "        filter_num = [None, 32, 64, 128, 256]\n",
    "        kernel_size = [None, 8, 8, 8, 8]\n",
    "        conv_stride_size = [None, 1, 1, 1, 1]\n",
    "        pool_stride_size = [None, 4, 4, 4, 4]\n",
    "        pool_size = [None, 8, 8, 8, 8]\n",
    "\n",
    "        for i in range(1, 4):\n",
    "            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n",
    "                             strides=conv_stride_size[i], padding='same',\n",
    "                             input_shape=input_shape if i == 1 else (None, input_shape[1]),\n",
    "                             name=f'block{i}_conv1'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i * 2 - 1}'))\n",
    "            model.add(ELU(alpha=1.0, name=f'block{i}_adv_act1'))\n",
    "\n",
    "            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n",
    "                             strides=conv_stride_size[i], padding='same',\n",
    "                             name=f'block{i}_conv2'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i * 2}'))\n",
    "            model.add(ELU(alpha=1.0, name=f'block{i}_adv_act2'))\n",
    "\n",
    "            model.add(MaxPooling1D(pool_size=pool_size[i], strides=pool_stride_size[i],\n",
    "                                   padding='same', name=f'block{i}_pool'))\n",
    "            model.add(Dropout(0.1, name=f'block{i}_dropout'))\n",
    "\n",
    "        # ... (rest of the model remains unchanged)\n",
    "\n",
    "\n",
    "        model.add(Flatten(name='flatten'))\n",
    "\n",
    "\n",
    "\n",
    "        # Fully connected layers\n",
    "        for i in range(1, 4):\n",
    "            model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name=f'fc{i}'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i + 8}'))\n",
    "            model.add(Activation('relu', name=f'fc{i}_act'))\n",
    "\n",
    "            # Experiment with different dropout rates\n",
    "            model.add(Dropout(dropout_rate_fc, name=f'fc{i}_dropout'))\n",
    "\n",
    "        # Output layer\n",
    "        #model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc3'))\n",
    "       # model.add(Activation('softmax', name=\"softmax\"))\n",
    "        # Output layer\n",
    "        model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc_final'))\n",
    "        model.add(Activation('softmax', name=\"softmax\"))\n",
    "\n",
    "        print(\"Model built successfully.\")\n",
    "        return model\n",
    "\n",
    "# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\n",
    "input_shape = (5000, 1)\n",
    "num_classes = 10\n",
    "\n",
    "# Experiment with different dropout rates\n",
    "dropout_rate1 = 0.6\n",
    "dropout_rate2 = 0.4\n",
    "dropout_rate_fc = 0.5\n",
    "\n",
    "# Build the model with the specified dropout rates\n",
    "model = DFNet.build(input_shape=input_shape, classes=num_classes,\n",
    "                    dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n",
    "\n",
    "\n",
    "# Print the model summary\n",
    "print(\"Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\n",
    "input_shape = (5000, 1)\n",
    "num_classes = 95\n",
    "\n",
    "# Experiment with different dropout rates\n",
    "dropout_rate1 = 0.6\n",
    "dropout_rate2 = 0.4\n",
    "dropout_rate_fc = 0.5\n",
    "\n",
    "# Load and merge data using the function you defined\n",
    "X_all, y_all = LoadDataNoDefCW()\n",
    "\n",
    "# Define the number of folds\n",
    "n_splits = 5  # You can adjust this as needed\n",
    "\n",
    "# Initialize Stratified K-Fold\n",
    "stratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results\n",
    "all_histories = []\n",
    "\n",
    "# Iterate over folds\n",
    "for fold, (train_index, test_index) in enumerate(stratkf.split(X_all, y_all)):\n",
    "    print(f\"\\nTraining on Fold {fold + 1}/{n_splits}\")\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test = X_all[train_index], X_all[test_index]\n",
    "    y_train, y_test = y_all[train_index], y_all[test_index]\n",
    "\n",
    "    # Build the model\n",
    "    model = DFNet.build(input_shape=input_shape, classes=num_classes,\n",
    "                        dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(f\"\\nFold {fold + 1} - Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Save history for later analysis if needed\n",
    "    all_histories.append(history)\n",
    "    # Check unique labels in y_train\n",
    "    \n",
    "    # Update num_classes based on the actual number of unique classes in your dataset\n",
    "num_classes = len(np.unique(y_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f69a1",
   "metadata": {
    "papermill": {
     "duration": 0.023473,
     "end_time": "2024-01-06T12:29:56.012286",
     "exception": false,
     "start_time": "2024-01-06T12:29:55.988813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133a81c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T12:29:56.061584Z",
     "iopub.status.busy": "2024-01-06T12:29:56.061267Z",
     "iopub.status.idle": "2024-01-06T12:30:02.197594Z",
     "shell.execute_reply": "2024-01-06T12:30:02.196418Z"
    },
    "papermill": {
     "duration": 6.163475,
     "end_time": "2024-01-06T12:30:02.199891",
     "exception": false,
     "start_time": "2024-01-06T12:29:56.036416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading non-defended dataset for closed-world scenario\n",
      "Dataset directory: /kaggle/input/dataset-non-defended-close-world/\n",
      "X_train loaded\n",
      "y_train loaded\n",
      "X_valid loaded\n",
      "y_valid loaded\n",
      "X_test loaded\n",
      "y_test loaded\n",
      "Data dimensions:\n",
      "X: Training data's shape :  (76000, 5000)\n",
      "y: Training data's shape :  (76000,)\n",
      "X: Validation data's shape :  (9500, 5000)\n",
      "y: Validation data's shape :  (9500,)\n",
      "X: Testing data's shape :  (9500, 5000)\n",
      "y: Testing data's shape :  (9500,)\n",
      "Merged data dimensions:\n",
      "X: Merged data's shape :  (95000, 5000)\n",
      "y: Merged data's shape :  (95000,)\n",
      "Features of the merged dataset:\n",
      "[[ 1. -1. -1. ... -1. -1. -1.]\n",
      " [-1.  1.  1. ...  0.  0.  0.]\n",
      " [ 1. -1. -1. ...  0.  0.  0.]\n",
      " ...\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [ 1. -1.  1. ...  0.  0.  0.]]\n",
      "Class distribution:\n",
      "Class 0: 1000 samples\n",
      "Class 1: 1000 samples\n",
      "Class 2: 1000 samples\n",
      "Class 3: 1000 samples\n",
      "Class 4: 1000 samples\n",
      "Class 5: 1000 samples\n",
      "Class 6: 1000 samples\n",
      "Class 7: 1000 samples\n",
      "Class 8: 1000 samples\n",
      "Class 9: 1000 samples\n",
      "Class 10: 1000 samples\n",
      "Class 11: 1000 samples\n",
      "Class 12: 1000 samples\n",
      "Class 13: 1000 samples\n",
      "Class 14: 1000 samples\n",
      "Class 15: 1000 samples\n",
      "Class 16: 1000 samples\n",
      "Class 17: 1000 samples\n",
      "Class 18: 1000 samples\n",
      "Class 19: 1000 samples\n",
      "Class 20: 1000 samples\n",
      "Class 21: 1000 samples\n",
      "Class 22: 1000 samples\n",
      "Class 23: 1000 samples\n",
      "Class 24: 1000 samples\n",
      "Class 25: 1000 samples\n",
      "Class 26: 1000 samples\n",
      "Class 27: 1000 samples\n",
      "Class 28: 1000 samples\n",
      "Class 29: 1000 samples\n",
      "Class 30: 1000 samples\n",
      "Class 31: 1000 samples\n",
      "Class 32: 1000 samples\n",
      "Class 33: 1000 samples\n",
      "Class 34: 1000 samples\n",
      "Class 35: 1000 samples\n",
      "Class 36: 1000 samples\n",
      "Class 37: 1000 samples\n",
      "Class 38: 1000 samples\n",
      "Class 39: 1000 samples\n",
      "Class 40: 1000 samples\n",
      "Class 41: 1000 samples\n",
      "Class 42: 1000 samples\n",
      "Class 43: 1000 samples\n",
      "Class 44: 1000 samples\n",
      "Class 45: 1000 samples\n",
      "Class 46: 1000 samples\n",
      "Class 47: 1000 samples\n",
      "Class 48: 1000 samples\n",
      "Class 49: 1000 samples\n",
      "Class 50: 1000 samples\n",
      "Class 51: 1000 samples\n",
      "Class 52: 1000 samples\n",
      "Class 53: 1000 samples\n",
      "Class 54: 1000 samples\n",
      "Class 55: 1000 samples\n",
      "Class 56: 1000 samples\n",
      "Class 57: 1000 samples\n",
      "Class 58: 1000 samples\n",
      "Class 59: 1000 samples\n",
      "Class 60: 1000 samples\n",
      "Class 61: 1000 samples\n",
      "Class 62: 1000 samples\n",
      "Class 63: 1000 samples\n",
      "Class 64: 1000 samples\n",
      "Class 65: 1000 samples\n",
      "Class 66: 1000 samples\n",
      "Class 67: 1000 samples\n",
      "Class 68: 1000 samples\n",
      "Class 69: 1000 samples\n",
      "Class 70: 1000 samples\n",
      "Class 71: 1000 samples\n",
      "Class 72: 1000 samples\n",
      "Class 73: 1000 samples\n",
      "Class 74: 1000 samples\n",
      "Class 75: 1000 samples\n",
      "Class 76: 1000 samples\n",
      "Class 77: 1000 samples\n",
      "Class 78: 1000 samples\n",
      "Class 79: 1000 samples\n",
      "Class 80: 1000 samples\n",
      "Class 81: 1000 samples\n",
      "Class 82: 1000 samples\n",
      "Class 83: 1000 samples\n",
      "Class 84: 1000 samples\n",
      "Class 85: 1000 samples\n",
      "Class 86: 1000 samples\n",
      "Class 87: 1000 samples\n",
      "Class 88: 1000 samples\n",
      "Class 89: 1000 samples\n",
      "Class 90: 1000 samples\n",
      "Class 91: 1000 samples\n",
      "Class 92: 1000 samples\n",
      "Class 93: 1000 samples\n",
      "Class 94: 1000 samples\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout, ELU\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from keras import layers, models\n",
    "\n",
    "# ... (your existing code)\n",
    "def LoadDataNoDefCW():\n",
    "\n",
    "    print(\"Loading non-defended dataset for closed-world scenario\")\n",
    "\n",
    "    # Point to the directory storing data\n",
    "    dataset_dir = '/kaggle/input/dataset-non-defended-close-world/'\n",
    "\n",
    "    # Debug: Print dataset directory\n",
    "    print(\"Dataset directory:\", dataset_dir)\n",
    "\n",
    "    try:\n",
    "        # Load training data\n",
    "        with open(dataset_dir + 'X_train_NoDef.pkl', 'rb') as handle:\n",
    "            X_train = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"X_train loaded\")\n",
    "\n",
    "        with open(dataset_dir + 'y_train_NoDef.pkl', 'rb') as handle:\n",
    "            y_train = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"y_train loaded\")\n",
    "\n",
    "        # Load validation data\n",
    "        with open(dataset_dir + 'X_valid_NoDef.pkl', 'rb') as handle:\n",
    "            X_valid = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"X_valid loaded\")\n",
    "\n",
    "        with open(dataset_dir + 'y_valid_NoDef.pkl', 'rb') as handle:\n",
    "            y_valid = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"y_valid loaded\")\n",
    "\n",
    "        # Load testing data\n",
    "        with open(dataset_dir + 'X_test_NoDef.pkl', 'rb') as handle:\n",
    "            X_test = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"X_test loaded\")\n",
    "\n",
    "        with open(dataset_dir + 'y_test_NoDef.pkl', 'rb') as handle:\n",
    "            y_test = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"y_test loaded\")\n",
    "\n",
    "        print(\"Data dimensions:\")\n",
    "        print(\"X: Training data's shape : \", X_train.shape)\n",
    "        print(\"y: Training data's shape : \", y_train.shape)\n",
    "        print(\"X: Validation data's shape : \", X_valid.shape)\n",
    "        print(\"y: Validation data's shape : \", y_valid.shape)\n",
    "        print(\"X: Testing data's shape : \", X_test.shape)\n",
    "        print(\"y: Testing data's shape : \", y_test.shape)\n",
    "\n",
    "        # Merge datasets\n",
    "        X_all = np.concatenate((X_train, X_valid, X_test), axis=0)\n",
    "        y_all = np.concatenate((y_train, y_valid, y_test), axis=0)\n",
    "\n",
    "        print(\"Merged data dimensions:\")\n",
    "        print(\"X: Merged data's shape : \", X_all.shape)\n",
    "        print(\"y: Merged data's shape : \", y_all.shape)\n",
    "        \n",
    "        # Print features of the merged dataset\n",
    "        print(\"Features of the merged dataset:\")\n",
    "        print(X_all)\n",
    "        \n",
    "        # Check if the class distribution is balanced\n",
    "        unique_classes, class_counts = np.unique(y_all, return_counts=True)\n",
    "        class_distribution = dict(zip(unique_classes, class_counts))\n",
    "\n",
    "        print(\"Class distribution:\")\n",
    "        for class_label, count in class_distribution.items():\n",
    "            print(f\"Class {class_label}: {count} samples\")\n",
    "\n",
    "        # Plot the class distribution\n",
    "        plt.bar(class_distribution.keys(), class_distribution.values())\n",
    "        plt.xlabel('Class Label')\n",
    "        plt.ylabel('Number of Samples')\n",
    "        plt.title('Class Distribution')\n",
    "        plt.show()\n",
    "        \n",
    "        return X_all, y_all\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        return None\n",
    "\n",
    "# Call the function to load and merge data\n",
    "X_all, y_all = LoadDataNoDefCW()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40b6d3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T12:30:02.252689Z",
     "iopub.status.busy": "2024-01-06T12:30:02.252326Z",
     "iopub.status.idle": "2024-01-06T14:29:12.669940Z",
     "shell.execute_reply": "2024-01-06T14:29:12.668646Z"
    },
    "papermill": {
     "duration": 7150.446507,
     "end_time": "2024-01-06T14:29:12.672624",
     "exception": false,
     "start_time": "2024-01-06T12:30:02.226117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built successfully.\n",
      "Model Summary:\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv1D)       (None, 5000, 32)          288       \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 5000, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block1_adv_act1 (ELU)       (None, 5000, 32)          0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv1D)       (None, 5000, 32)          8224      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 5000, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block1_adv_act2 (ELU)       (None, 5000, 32)          0         \n",
      "                                                                 \n",
      " block1_pool (MaxPooling1D)  (None, 1250, 32)          0         \n",
      "                                                                 \n",
      " block1_dropout (Dropout)    (None, 1250, 32)          0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv1D)       (None, 1250, 64)          16448     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 1250, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block2_adv_act1 (ELU)       (None, 1250, 64)          0         \n",
      "                                                                 \n",
      " block2_conv2 (Conv1D)       (None, 1250, 64)          32832     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 1250, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block2_adv_act2 (ELU)       (None, 1250, 64)          0         \n",
      "                                                                 \n",
      " block2_pool (MaxPooling1D)  (None, 313, 64)           0         \n",
      "                                                                 \n",
      " block2_dropout (Dropout)    (None, 313, 64)           0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv1D)       (None, 313, 128)          65664     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 313, 128)          512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block3_adv_act1 (ELU)       (None, 313, 128)          0         \n",
      "                                                                 \n",
      " block3_conv2 (Conv1D)       (None, 313, 128)          131200    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 313, 128)          512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block3_adv_act2 (ELU)       (None, 313, 128)          0         \n",
      "                                                                 \n",
      " block3_pool (MaxPooling1D)  (None, 79, 128)           0         \n",
      "                                                                 \n",
      " block3_dropout (Dropout)    (None, 79, 128)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10112)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 512)               5177856   \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " fc1_act (Activation)        (None, 512)               0         \n",
      "                                                                 \n",
      " fc1_dropout (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " fc2_act (Activation)        (None, 512)               0         \n",
      "                                                                 \n",
      " fc2_dropout (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " fc3 (Dense)                 (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " fc3_act (Activation)        (None, 512)               0         \n",
      "                                                                 \n",
      " fc3_dropout (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " fc_final (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5970890 (22.78 MB)\n",
      "Trainable params: 5966922 (22.76 MB)\n",
      "Non-trainable params: 3968 (15.50 KB)\n",
      "_________________________________________________________________\n",
      "Loading non-defended dataset for closed-world scenario\n",
      "Dataset directory: /kaggle/input/dataset-non-defended-close-world/\n",
      "X_train loaded\n",
      "y_train loaded\n",
      "X_valid loaded\n",
      "y_valid loaded\n",
      "X_test loaded\n",
      "y_test loaded\n",
      "Data dimensions:\n",
      "X: Training data's shape :  (76000, 5000)\n",
      "y: Training data's shape :  (76000,)\n",
      "X: Validation data's shape :  (9500, 5000)\n",
      "y: Validation data's shape :  (9500,)\n",
      "X: Testing data's shape :  (9500, 5000)\n",
      "y: Testing data's shape :  (9500,)\n",
      "Merged data dimensions:\n",
      "X: Merged data's shape :  (95000, 5000)\n",
      "y: Merged data's shape :  (95000,)\n",
      "Features of the merged dataset:\n",
      "[[ 1. -1. -1. ... -1. -1. -1.]\n",
      " [-1.  1.  1. ...  0.  0.  0.]\n",
      " [ 1. -1. -1. ...  0.  0.  0.]\n",
      " ...\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [ 1. -1.  1. ...  0.  0.  0.]]\n",
      "Class distribution:\n",
      "Class 0: 1000 samples\n",
      "Class 1: 1000 samples\n",
      "Class 2: 1000 samples\n",
      "Class 3: 1000 samples\n",
      "Class 4: 1000 samples\n",
      "Class 5: 1000 samples\n",
      "Class 6: 1000 samples\n",
      "Class 7: 1000 samples\n",
      "Class 8: 1000 samples\n",
      "Class 9: 1000 samples\n",
      "Class 10: 1000 samples\n",
      "Class 11: 1000 samples\n",
      "Class 12: 1000 samples\n",
      "Class 13: 1000 samples\n",
      "Class 14: 1000 samples\n",
      "Class 15: 1000 samples\n",
      "Class 16: 1000 samples\n",
      "Class 17: 1000 samples\n",
      "Class 18: 1000 samples\n",
      "Class 19: 1000 samples\n",
      "Class 20: 1000 samples\n",
      "Class 21: 1000 samples\n",
      "Class 22: 1000 samples\n",
      "Class 23: 1000 samples\n",
      "Class 24: 1000 samples\n",
      "Class 25: 1000 samples\n",
      "Class 26: 1000 samples\n",
      "Class 27: 1000 samples\n",
      "Class 28: 1000 samples\n",
      "Class 29: 1000 samples\n",
      "Class 30: 1000 samples\n",
      "Class 31: 1000 samples\n",
      "Class 32: 1000 samples\n",
      "Class 33: 1000 samples\n",
      "Class 34: 1000 samples\n",
      "Class 35: 1000 samples\n",
      "Class 36: 1000 samples\n",
      "Class 37: 1000 samples\n",
      "Class 38: 1000 samples\n",
      "Class 39: 1000 samples\n",
      "Class 40: 1000 samples\n",
      "Class 41: 1000 samples\n",
      "Class 42: 1000 samples\n",
      "Class 43: 1000 samples\n",
      "Class 44: 1000 samples\n",
      "Class 45: 1000 samples\n",
      "Class 46: 1000 samples\n",
      "Class 47: 1000 samples\n",
      "Class 48: 1000 samples\n",
      "Class 49: 1000 samples\n",
      "Class 50: 1000 samples\n",
      "Class 51: 1000 samples\n",
      "Class 52: 1000 samples\n",
      "Class 53: 1000 samples\n",
      "Class 54: 1000 samples\n",
      "Class 55: 1000 samples\n",
      "Class 56: 1000 samples\n",
      "Class 57: 1000 samples\n",
      "Class 58: 1000 samples\n",
      "Class 59: 1000 samples\n",
      "Class 60: 1000 samples\n",
      "Class 61: 1000 samples\n",
      "Class 62: 1000 samples\n",
      "Class 63: 1000 samples\n",
      "Class 64: 1000 samples\n",
      "Class 65: 1000 samples\n",
      "Class 66: 1000 samples\n",
      "Class 67: 1000 samples\n",
      "Class 68: 1000 samples\n",
      "Class 69: 1000 samples\n",
      "Class 70: 1000 samples\n",
      "Class 71: 1000 samples\n",
      "Class 72: 1000 samples\n",
      "Class 73: 1000 samples\n",
      "Class 74: 1000 samples\n",
      "Class 75: 1000 samples\n",
      "Class 76: 1000 samples\n",
      "Class 77: 1000 samples\n",
      "Class 78: 1000 samples\n",
      "Class 79: 1000 samples\n",
      "Class 80: 1000 samples\n",
      "Class 81: 1000 samples\n",
      "Class 82: 1000 samples\n",
      "Class 83: 1000 samples\n",
      "Class 84: 1000 samples\n",
      "Class 85: 1000 samples\n",
      "Class 86: 1000 samples\n",
      "Class 87: 1000 samples\n",
      "Class 88: 1000 samples\n",
      "Class 89: 1000 samples\n",
      "Class 90: 1000 samples\n",
      "Class 91: 1000 samples\n",
      "Class 92: 1000 samples\n",
      "Class 93: 1000 samples\n",
      "Class 94: 1000 samples\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on Fold 1/5\n",
      "Model built successfully.\n",
      "Epoch 1/20\n",
      "1188/1188 - 90s - loss: 1.3757 - accuracy: 0.6466 - val_loss: 0.3370 - val_accuracy: 0.9181 - 90s/epoch - 75ms/step\n",
      "Epoch 2/20\n",
      "1188/1188 - 80s - loss: 0.4984 - accuracy: 0.8709 - val_loss: 0.2487 - val_accuracy: 0.9413 - 80s/epoch - 68ms/step\n",
      "Epoch 3/20\n",
      "1188/1188 - 80s - loss: 0.3645 - accuracy: 0.9056 - val_loss: 0.1801 - val_accuracy: 0.9553 - 80s/epoch - 68ms/step\n",
      "Epoch 4/20\n",
      "1188/1188 - 81s - loss: 0.3026 - accuracy: 0.9213 - val_loss: 0.1638 - val_accuracy: 0.9599 - 81s/epoch - 68ms/step\n",
      "Epoch 5/20\n",
      "1188/1188 - 81s - loss: 0.2566 - accuracy: 0.9330 - val_loss: 0.1489 - val_accuracy: 0.9629 - 81s/epoch - 68ms/step\n",
      "Epoch 6/20\n",
      "1188/1188 - 81s - loss: 0.2293 - accuracy: 0.9398 - val_loss: 0.1384 - val_accuracy: 0.9655 - 81s/epoch - 68ms/step\n",
      "Epoch 7/20\n",
      "1188/1188 - 80s - loss: 0.2053 - accuracy: 0.9455 - val_loss: 0.1237 - val_accuracy: 0.9696 - 80s/epoch - 68ms/step\n",
      "Epoch 8/20\n",
      "1188/1188 - 80s - loss: 0.1889 - accuracy: 0.9499 - val_loss: 0.1238 - val_accuracy: 0.9703 - 80s/epoch - 68ms/step\n",
      "Epoch 9/20\n",
      "1188/1188 - 80s - loss: 0.1728 - accuracy: 0.9534 - val_loss: 0.1166 - val_accuracy: 0.9728 - 80s/epoch - 68ms/step\n",
      "Epoch 10/20\n",
      "1188/1188 - 80s - loss: 0.1564 - accuracy: 0.9577 - val_loss: 0.1128 - val_accuracy: 0.9718 - 80s/epoch - 68ms/step\n",
      "Epoch 11/20\n",
      "1188/1188 - 80s - loss: 0.1493 - accuracy: 0.9604 - val_loss: 0.1108 - val_accuracy: 0.9730 - 80s/epoch - 68ms/step\n",
      "Epoch 12/20\n",
      "1188/1188 - 80s - loss: 0.1434 - accuracy: 0.9612 - val_loss: 0.1116 - val_accuracy: 0.9745 - 80s/epoch - 68ms/step\n",
      "Epoch 13/20\n",
      "1188/1188 - 80s - loss: 0.1302 - accuracy: 0.9646 - val_loss: 0.1080 - val_accuracy: 0.9746 - 80s/epoch - 68ms/step\n",
      "Epoch 14/20\n",
      "1188/1188 - 80s - loss: 0.1269 - accuracy: 0.9654 - val_loss: 0.1263 - val_accuracy: 0.9685 - 80s/epoch - 68ms/step\n",
      "Epoch 15/20\n",
      "1188/1188 - 80s - loss: 0.1193 - accuracy: 0.9677 - val_loss: 0.1044 - val_accuracy: 0.9762 - 80s/epoch - 68ms/step\n",
      "Epoch 16/20\n",
      "1188/1188 - 80s - loss: 0.1140 - accuracy: 0.9682 - val_loss: 0.0997 - val_accuracy: 0.9767 - 80s/epoch - 68ms/step\n",
      "Epoch 17/20\n",
      "1188/1188 - 80s - loss: 0.1094 - accuracy: 0.9703 - val_loss: 0.1104 - val_accuracy: 0.9756 - 80s/epoch - 68ms/step\n",
      "Epoch 18/20\n",
      "1188/1188 - 80s - loss: 0.1090 - accuracy: 0.9710 - val_loss: 0.1014 - val_accuracy: 0.9780 - 80s/epoch - 68ms/step\n",
      "Epoch 19/20\n",
      "1188/1188 - 80s - loss: 0.1001 - accuracy: 0.9725 - val_loss: 0.1044 - val_accuracy: 0.9763 - 80s/epoch - 68ms/step\n",
      "594/594 - 6s - loss: 0.0997 - accuracy: 0.9767 - 6s/epoch - 9ms/step\n",
      "\n",
      "Fold 1 - Test Accuracy: 97.67%\n",
      "\n",
      "Training on Fold 2/5\n",
      "Model built successfully.\n",
      "Epoch 1/20\n",
      "1188/1188 - 90s - loss: 1.4886 - accuracy: 0.6166 - val_loss: 0.3621 - val_accuracy: 0.9089 - 90s/epoch - 75ms/step\n",
      "Epoch 2/20\n",
      "1188/1188 - 80s - loss: 0.5238 - accuracy: 0.8630 - val_loss: 0.2262 - val_accuracy: 0.9435 - 80s/epoch - 68ms/step\n",
      "Epoch 3/20\n",
      "1188/1188 - 80s - loss: 0.3815 - accuracy: 0.9011 - val_loss: 0.1805 - val_accuracy: 0.9549 - 80s/epoch - 68ms/step\n",
      "Epoch 4/20\n",
      "1188/1188 - 80s - loss: 0.3095 - accuracy: 0.9186 - val_loss: 0.1561 - val_accuracy: 0.9612 - 80s/epoch - 68ms/step\n",
      "Epoch 5/20\n",
      "1188/1188 - 80s - loss: 0.2673 - accuracy: 0.9297 - val_loss: 0.1617 - val_accuracy: 0.9601 - 80s/epoch - 68ms/step\n",
      "Epoch 6/20\n",
      "1188/1188 - 80s - loss: 0.2345 - accuracy: 0.9378 - val_loss: 0.1306 - val_accuracy: 0.9697 - 80s/epoch - 68ms/step\n",
      "Epoch 7/20\n",
      "1188/1188 - 81s - loss: 0.2117 - accuracy: 0.9446 - val_loss: 0.1174 - val_accuracy: 0.9716 - 81s/epoch - 68ms/step\n",
      "Epoch 8/20\n",
      "1188/1188 - 80s - loss: 0.1903 - accuracy: 0.9491 - val_loss: 0.1216 - val_accuracy: 0.9699 - 80s/epoch - 68ms/step\n",
      "Epoch 9/20\n",
      "1188/1188 - 80s - loss: 0.1794 - accuracy: 0.9526 - val_loss: 0.1202 - val_accuracy: 0.9699 - 80s/epoch - 68ms/step\n",
      "Epoch 10/20\n",
      "1188/1188 - 80s - loss: 0.1655 - accuracy: 0.9560 - val_loss: 0.1152 - val_accuracy: 0.9733 - 80s/epoch - 68ms/step\n",
      "Epoch 11/20\n",
      "1188/1188 - 80s - loss: 0.1505 - accuracy: 0.9593 - val_loss: 0.1119 - val_accuracy: 0.9720 - 80s/epoch - 68ms/step\n",
      "Epoch 12/20\n",
      "1188/1188 - 80s - loss: 0.1424 - accuracy: 0.9622 - val_loss: 0.1033 - val_accuracy: 0.9753 - 80s/epoch - 68ms/step\n",
      "Epoch 13/20\n",
      "1188/1188 - 80s - loss: 0.1365 - accuracy: 0.9637 - val_loss: 0.0972 - val_accuracy: 0.9766 - 80s/epoch - 68ms/step\n",
      "Epoch 14/20\n",
      "1188/1188 - 80s - loss: 0.1333 - accuracy: 0.9649 - val_loss: 0.1012 - val_accuracy: 0.9760 - 80s/epoch - 68ms/step\n",
      "Epoch 15/20\n",
      "1188/1188 - 80s - loss: 0.1204 - accuracy: 0.9674 - val_loss: 0.0962 - val_accuracy: 0.9774 - 80s/epoch - 68ms/step\n",
      "Epoch 16/20\n",
      "1188/1188 - 80s - loss: 0.1167 - accuracy: 0.9679 - val_loss: 0.1197 - val_accuracy: 0.9714 - 80s/epoch - 68ms/step\n",
      "Epoch 17/20\n",
      "1188/1188 - 80s - loss: 0.1119 - accuracy: 0.9699 - val_loss: 0.1031 - val_accuracy: 0.9767 - 80s/epoch - 68ms/step\n",
      "Epoch 18/20\n",
      "1188/1188 - 80s - loss: 0.1112 - accuracy: 0.9702 - val_loss: 0.0966 - val_accuracy: 0.9775 - 80s/epoch - 68ms/step\n",
      "594/594 - 6s - loss: 0.0962 - accuracy: 0.9774 - 6s/epoch - 10ms/step\n",
      "\n",
      "Fold 2 - Test Accuracy: 97.74%\n",
      "\n",
      "Training on Fold 3/5\n",
      "Model built successfully.\n",
      "Epoch 1/20\n",
      "1188/1188 - 91s - loss: 1.4330 - accuracy: 0.6318 - val_loss: 0.3646 - val_accuracy: 0.9144 - 91s/epoch - 76ms/step\n",
      "Epoch 2/20\n",
      "1188/1188 - 81s - loss: 0.5165 - accuracy: 0.8660 - val_loss: 0.2402 - val_accuracy: 0.9413 - 81s/epoch - 68ms/step\n",
      "Epoch 3/20\n",
      "1188/1188 - 81s - loss: 0.3766 - accuracy: 0.9028 - val_loss: 0.1892 - val_accuracy: 0.9532 - 81s/epoch - 68ms/step\n",
      "Epoch 4/20\n",
      "1188/1188 - 81s - loss: 0.3042 - accuracy: 0.9205 - val_loss: 0.1829 - val_accuracy: 0.9553 - 81s/epoch - 68ms/step\n",
      "Epoch 5/20\n",
      "1188/1188 - 80s - loss: 0.2604 - accuracy: 0.9313 - val_loss: 0.1676 - val_accuracy: 0.9602 - 80s/epoch - 68ms/step\n",
      "Epoch 6/20\n",
      "1188/1188 - 81s - loss: 0.2312 - accuracy: 0.9385 - val_loss: 0.1409 - val_accuracy: 0.9657 - 81s/epoch - 68ms/step\n",
      "Epoch 7/20\n",
      "1188/1188 - 81s - loss: 0.2058 - accuracy: 0.9449 - val_loss: 0.1292 - val_accuracy: 0.9673 - 81s/epoch - 68ms/step\n",
      "Epoch 8/20\n",
      "1188/1188 - 81s - loss: 0.1881 - accuracy: 0.9499 - val_loss: 0.1236 - val_accuracy: 0.9689 - 81s/epoch - 68ms/step\n",
      "Epoch 9/20\n",
      "1188/1188 - 81s - loss: 0.1749 - accuracy: 0.9535 - val_loss: 0.1217 - val_accuracy: 0.9695 - 81s/epoch - 68ms/step\n",
      "Epoch 10/20\n",
      "1188/1188 - 81s - loss: 0.1612 - accuracy: 0.9568 - val_loss: 0.1126 - val_accuracy: 0.9717 - 81s/epoch - 68ms/step\n",
      "Epoch 11/20\n",
      "1188/1188 - 81s - loss: 0.1505 - accuracy: 0.9592 - val_loss: 0.1136 - val_accuracy: 0.9722 - 81s/epoch - 68ms/step\n",
      "Epoch 12/20\n",
      "1188/1188 - 81s - loss: 0.1431 - accuracy: 0.9617 - val_loss: 0.1111 - val_accuracy: 0.9734 - 81s/epoch - 68ms/step\n",
      "Epoch 13/20\n",
      "1188/1188 - 81s - loss: 0.1339 - accuracy: 0.9639 - val_loss: 0.1135 - val_accuracy: 0.9731 - 81s/epoch - 68ms/step\n",
      "Epoch 14/20\n",
      "1188/1188 - 81s - loss: 0.1270 - accuracy: 0.9661 - val_loss: 0.1119 - val_accuracy: 0.9732 - 81s/epoch - 68ms/step\n",
      "Epoch 15/20\n",
      "1188/1188 - 81s - loss: 0.1256 - accuracy: 0.9659 - val_loss: 0.1061 - val_accuracy: 0.9737 - 81s/epoch - 68ms/step\n",
      "Epoch 16/20\n",
      "1188/1188 - 81s - loss: 0.1167 - accuracy: 0.9687 - val_loss: 0.1099 - val_accuracy: 0.9738 - 81s/epoch - 68ms/step\n",
      "Epoch 17/20\n",
      "1188/1188 - 81s - loss: 0.1125 - accuracy: 0.9701 - val_loss: 0.1029 - val_accuracy: 0.9762 - 81s/epoch - 68ms/step\n",
      "Epoch 18/20\n",
      "1188/1188 - 81s - loss: 0.1040 - accuracy: 0.9717 - val_loss: 0.1003 - val_accuracy: 0.9759 - 81s/epoch - 68ms/step\n",
      "Epoch 19/20\n",
      "1188/1188 - 81s - loss: 0.1018 - accuracy: 0.9718 - val_loss: 0.0991 - val_accuracy: 0.9759 - 81s/epoch - 68ms/step\n",
      "Epoch 20/20\n",
      "1188/1188 - 81s - loss: 0.0996 - accuracy: 0.9734 - val_loss: 0.0996 - val_accuracy: 0.9768 - 81s/epoch - 68ms/step\n",
      "594/594 - 6s - loss: 0.0996 - accuracy: 0.9768 - 6s/epoch - 9ms/step\n",
      "\n",
      "Fold 3 - Test Accuracy: 97.68%\n",
      "\n",
      "Training on Fold 4/5\n",
      "Model built successfully.\n",
      "Epoch 1/20\n",
      "1188/1188 - 90s - loss: 1.4160 - accuracy: 0.6358 - val_loss: 0.3780 - val_accuracy: 0.9043 - 90s/epoch - 76ms/step\n",
      "Epoch 2/20\n",
      "1188/1188 - 81s - loss: 0.5105 - accuracy: 0.8675 - val_loss: 0.2164 - val_accuracy: 0.9450 - 81s/epoch - 68ms/step\n",
      "Epoch 3/20\n",
      "1188/1188 - 81s - loss: 0.3820 - accuracy: 0.9010 - val_loss: 0.1975 - val_accuracy: 0.9506 - 81s/epoch - 68ms/step\n",
      "Epoch 4/20\n",
      "1188/1188 - 81s - loss: 0.3048 - accuracy: 0.9198 - val_loss: 0.1661 - val_accuracy: 0.9572 - 81s/epoch - 68ms/step\n",
      "Epoch 5/20\n",
      "1188/1188 - 81s - loss: 0.2626 - accuracy: 0.9316 - val_loss: 0.1511 - val_accuracy: 0.9614 - 81s/epoch - 68ms/step\n",
      "Epoch 6/20\n",
      "1188/1188 - 80s - loss: 0.2321 - accuracy: 0.9380 - val_loss: 0.1635 - val_accuracy: 0.9571 - 80s/epoch - 68ms/step\n",
      "Epoch 7/20\n",
      "1188/1188 - 81s - loss: 0.2114 - accuracy: 0.9436 - val_loss: 0.1258 - val_accuracy: 0.9686 - 81s/epoch - 68ms/step\n",
      "Epoch 8/20\n",
      "1188/1188 - 81s - loss: 0.1894 - accuracy: 0.9501 - val_loss: 0.1139 - val_accuracy: 0.9711 - 81s/epoch - 68ms/step\n",
      "Epoch 9/20\n",
      "1188/1188 - 81s - loss: 0.1726 - accuracy: 0.9542 - val_loss: 0.1150 - val_accuracy: 0.9719 - 81s/epoch - 68ms/step\n",
      "Epoch 10/20\n",
      "1188/1188 - 81s - loss: 0.1579 - accuracy: 0.9578 - val_loss: 0.1163 - val_accuracy: 0.9705 - 81s/epoch - 68ms/step\n",
      "Epoch 11/20\n",
      "1188/1188 - 81s - loss: 0.1478 - accuracy: 0.9606 - val_loss: 0.1148 - val_accuracy: 0.9720 - 81s/epoch - 68ms/step\n",
      "594/594 - 6s - loss: 0.1139 - accuracy: 0.9711 - 6s/epoch - 10ms/step\n",
      "\n",
      "Fold 4 - Test Accuracy: 97.11%\n",
      "\n",
      "Training on Fold 5/5\n",
      "Model built successfully.\n",
      "Epoch 1/20\n",
      "1188/1188 - 90s - loss: 1.4504 - accuracy: 0.6269 - val_loss: 0.3548 - val_accuracy: 0.9106 - 90s/epoch - 76ms/step\n",
      "Epoch 2/20\n",
      "1188/1188 - 81s - loss: 0.5095 - accuracy: 0.8681 - val_loss: 0.2341 - val_accuracy: 0.9446 - 81s/epoch - 68ms/step\n",
      "Epoch 3/20\n",
      "1188/1188 - 81s - loss: 0.3739 - accuracy: 0.9013 - val_loss: 0.1852 - val_accuracy: 0.9525 - 81s/epoch - 68ms/step\n",
      "Epoch 4/20\n",
      "1188/1188 - 81s - loss: 0.2974 - accuracy: 0.9221 - val_loss: 0.1661 - val_accuracy: 0.9591 - 81s/epoch - 68ms/step\n",
      "Epoch 5/20\n",
      "1188/1188 - 81s - loss: 0.2631 - accuracy: 0.9300 - val_loss: 0.1410 - val_accuracy: 0.9653 - 81s/epoch - 68ms/step\n",
      "Epoch 6/20\n",
      "1188/1188 - 81s - loss: 0.2316 - accuracy: 0.9397 - val_loss: 0.1305 - val_accuracy: 0.9691 - 81s/epoch - 68ms/step\n",
      "Epoch 7/20\n",
      "1188/1188 - 81s - loss: 0.2122 - accuracy: 0.9443 - val_loss: 0.1148 - val_accuracy: 0.9727 - 81s/epoch - 68ms/step\n",
      "Epoch 8/20\n",
      "1188/1188 - 81s - loss: 0.1893 - accuracy: 0.9492 - val_loss: 0.1189 - val_accuracy: 0.9719 - 81s/epoch - 68ms/step\n",
      "Epoch 9/20\n",
      "1188/1188 - 81s - loss: 0.1758 - accuracy: 0.9531 - val_loss: 0.1209 - val_accuracy: 0.9699 - 81s/epoch - 68ms/step\n",
      "Epoch 10/20\n",
      "1188/1188 - 81s - loss: 0.1610 - accuracy: 0.9565 - val_loss: 0.1063 - val_accuracy: 0.9731 - 81s/epoch - 68ms/step\n",
      "Epoch 11/20\n",
      "1188/1188 - 81s - loss: 0.1520 - accuracy: 0.9593 - val_loss: 0.1078 - val_accuracy: 0.9755 - 81s/epoch - 68ms/step\n",
      "Epoch 12/20\n",
      "1188/1188 - 81s - loss: 0.1410 - accuracy: 0.9632 - val_loss: 0.1033 - val_accuracy: 0.9746 - 81s/epoch - 68ms/step\n",
      "Epoch 13/20\n",
      "1188/1188 - 81s - loss: 0.1322 - accuracy: 0.9642 - val_loss: 0.1004 - val_accuracy: 0.9762 - 81s/epoch - 68ms/step\n",
      "Epoch 14/20\n",
      "1188/1188 - 81s - loss: 0.1285 - accuracy: 0.9650 - val_loss: 0.0967 - val_accuracy: 0.9775 - 81s/epoch - 68ms/step\n",
      "Epoch 15/20\n",
      "1188/1188 - 81s - loss: 0.1202 - accuracy: 0.9675 - val_loss: 0.0948 - val_accuracy: 0.9768 - 81s/epoch - 68ms/step\n",
      "Epoch 16/20\n",
      "1188/1188 - 81s - loss: 0.1139 - accuracy: 0.9688 - val_loss: 0.0921 - val_accuracy: 0.9786 - 81s/epoch - 68ms/step\n",
      "Epoch 17/20\n",
      "1188/1188 - 81s - loss: 0.1122 - accuracy: 0.9698 - val_loss: 0.0970 - val_accuracy: 0.9763 - 81s/epoch - 68ms/step\n",
      "Epoch 18/20\n",
      "1188/1188 - 81s - loss: 0.1030 - accuracy: 0.9716 - val_loss: 0.0939 - val_accuracy: 0.9789 - 81s/epoch - 68ms/step\n",
      "Epoch 19/20\n",
      "1188/1188 - 81s - loss: 0.1028 - accuracy: 0.9723 - val_loss: 0.0939 - val_accuracy: 0.9785 - 81s/epoch - 68ms/step\n",
      "594/594 - 6s - loss: 0.0921 - accuracy: 0.9786 - 6s/epoch - 10ms/step\n",
      "\n",
      "Fold 5 - Test Accuracy: 97.86%\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# ... (previous imports and code remain unchanged)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout, ELU\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DFNet:\n",
    "    @staticmethod\n",
    "    def build(input_shape, classes, dropout_rate1=0.7, dropout_rate2=0.5, dropout_rate_fc=0.5):\n",
    "        model = Sequential()\n",
    "\n",
    "        # ... (previous model definition remains unchanged)\n",
    "        # Block 1\n",
    "        filter_num = [None, 32, 64, 128, 256]\n",
    "        kernel_size = [None, 8, 8, 8, 8]\n",
    "        conv_stride_size = [None, 1, 1, 1, 1]\n",
    "        pool_stride_size = [None, 4, 4, 4, 4]\n",
    "        pool_size = [None, 8, 8, 8, 8]\n",
    "\n",
    "        for i in range(1, 4):\n",
    "            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n",
    "                             strides=conv_stride_size[i], padding='same',\n",
    "                             input_shape=input_shape if i == 1 else (None, input_shape[1]),\n",
    "                             name=f'block{i}_conv1'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i * 2 - 1}'))\n",
    "            model.add(ELU(alpha=1.0, name=f'block{i}_adv_act1'))\n",
    "\n",
    "            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n",
    "                             strides=conv_stride_size[i], padding='same',\n",
    "                             name=f'block{i}_conv2'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i * 2}'))\n",
    "            model.add(ELU(alpha=1.0, name=f'block{i}_adv_act2'))\n",
    "\n",
    "            model.add(MaxPooling1D(pool_size=pool_size[i], strides=pool_stride_size[i],\n",
    "                                   padding='same', name=f'block{i}_pool'))\n",
    "            model.add(Dropout(0.1, name=f'block{i}_dropout'))\n",
    "\n",
    "        # ... (rest of the model remains unchanged)\n",
    "\n",
    "\n",
    "        model.add(Flatten(name='flatten'))\n",
    "\n",
    "\n",
    "\n",
    "        # Fully connected layers\n",
    "        for i in range(1, 4):\n",
    "            model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name=f'fc{i}'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i + 8}'))\n",
    "            model.add(Activation('relu', name=f'fc{i}_act'))\n",
    "\n",
    "            # Experiment with different dropout rates\n",
    "            model.add(Dropout(dropout_rate_fc, name=f'fc{i}_dropout'))\n",
    "\n",
    "        # Output layer\n",
    "        #model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc3'))\n",
    "       # model.add(Activation('softmax', name=\"softmax\"))\n",
    "        # Output layer\n",
    "        model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc_final'))\n",
    "        model.add(Activation('softmax', name=\"softmax\"))\n",
    "\n",
    "        print(\"Model built successfully.\")\n",
    "        return model\n",
    "\n",
    "# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\n",
    "input_shape = (5000, 1)\n",
    "num_classes = 10\n",
    "\n",
    "# Experiment with different dropout rates\n",
    "dropout_rate1 = 0.6\n",
    "dropout_rate2 = 0.4\n",
    "dropout_rate_fc = 0.5\n",
    "\n",
    "# Build the model with the specified dropout rates\n",
    "model = DFNet.build(input_shape=input_shape, classes=num_classes,\n",
    "                    dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n",
    "\n",
    "\n",
    "# Print the model summary\n",
    "print(\"Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\n",
    "input_shape = (5000, 1)\n",
    "num_classes = 95\n",
    "\n",
    "# Experiment with different dropout rates\n",
    "dropout_rate1 = 0.6\n",
    "dropout_rate2 = 0.4\n",
    "dropout_rate_fc = 0.5\n",
    "\n",
    "# Load and merge data using the function you defined\n",
    "X_all, y_all = LoadDataNoDefCW()\n",
    "\n",
    "# Define the number of folds\n",
    "n_splits = 5  # You can adjust this as needed\n",
    "\n",
    "# Initialize Stratified K-Fold\n",
    "stratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results\n",
    "all_histories = []\n",
    "\n",
    "# Iterate over folds\n",
    "for fold, (train_index, test_index) in enumerate(stratkf.split(X_all, y_all)):\n",
    "    print(f\"\\nTraining on Fold {fold + 1}/{n_splits}\")\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test = X_all[train_index], X_all[test_index]\n",
    "    y_train, y_test = y_all[train_index], y_all[test_index]\n",
    "\n",
    "    # Build the model\n",
    "    model = DFNet.build(input_shape=input_shape, classes=num_classes,\n",
    "                        dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    history = model.fit(X_train, y_train, epochs=20, batch_size=64, \n",
    "                        validation_data=(X_test, y_test), verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(f\"\\nFold {fold + 1} - Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Save history for later analysis if needed\n",
    "    all_histories.append(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a8632a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T14:29:12.785794Z",
     "iopub.status.busy": "2024-01-06T14:29:12.785385Z",
     "iopub.status.idle": "2024-01-06T14:29:32.664816Z",
     "shell.execute_reply": "2024-01-06T14:29:32.663690Z"
    },
    "papermill": {
     "duration": 19.93897,
     "end_time": "2024-01-06T14:29:32.667388",
     "exception": false,
     "start_time": "2024-01-06T14:29:12.728418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading non-defended dataset for closed-world scenario\n",
      "Dataset directory: /kaggle/input/dataset-non-defended-close-world/\n",
      "X_train loaded\n",
      "y_train loaded\n",
      "X_valid loaded\n",
      "y_valid loaded\n",
      "X_test loaded\n",
      "y_test loaded\n",
      "Data dimensions:\n",
      "X: Training data's shape :  (76000, 5000)\n",
      "y: Training data's shape :  (76000,)\n",
      "X: Validation data's shape :  (9500, 5000)\n",
      "y: Validation data's shape :  (9500,)\n",
      "X: Testing data's shape :  (9500, 5000)\n",
      "y: Testing data's shape :  (9500,)\n",
      "Merged data dimensions:\n",
      "X: Merged data's shape :  (95000, 5000)\n",
      "y: Merged data's shape :  (95000,)\n",
      "Features of the merged dataset:\n",
      "[[ 1. -1. -1. ... -1. -1. -1.]\n",
      " [-1.  1.  1. ...  0.  0.  0.]\n",
      " [ 1. -1. -1. ...  0.  0.  0.]\n",
      " ...\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [ 1. -1.  1. ...  0.  0.  0.]]\n",
      "Class distribution:\n",
      "Class 0: 1000 samples\n",
      "Class 1: 1000 samples\n",
      "Class 2: 1000 samples\n",
      "Class 3: 1000 samples\n",
      "Class 4: 1000 samples\n",
      "Class 5: 1000 samples\n",
      "Class 6: 1000 samples\n",
      "Class 7: 1000 samples\n",
      "Class 8: 1000 samples\n",
      "Class 9: 1000 samples\n",
      "Class 10: 1000 samples\n",
      "Class 11: 1000 samples\n",
      "Class 12: 1000 samples\n",
      "Class 13: 1000 samples\n",
      "Class 14: 1000 samples\n",
      "Class 15: 1000 samples\n",
      "Class 16: 1000 samples\n",
      "Class 17: 1000 samples\n",
      "Class 18: 1000 samples\n",
      "Class 19: 1000 samples\n",
      "Class 20: 1000 samples\n",
      "Class 21: 1000 samples\n",
      "Class 22: 1000 samples\n",
      "Class 23: 1000 samples\n",
      "Class 24: 1000 samples\n",
      "Class 25: 1000 samples\n",
      "Class 26: 1000 samples\n",
      "Class 27: 1000 samples\n",
      "Class 28: 1000 samples\n",
      "Class 29: 1000 samples\n",
      "Class 30: 1000 samples\n",
      "Class 31: 1000 samples\n",
      "Class 32: 1000 samples\n",
      "Class 33: 1000 samples\n",
      "Class 34: 1000 samples\n",
      "Class 35: 1000 samples\n",
      "Class 36: 1000 samples\n",
      "Class 37: 1000 samples\n",
      "Class 38: 1000 samples\n",
      "Class 39: 1000 samples\n",
      "Class 40: 1000 samples\n",
      "Class 41: 1000 samples\n",
      "Class 42: 1000 samples\n",
      "Class 43: 1000 samples\n",
      "Class 44: 1000 samples\n",
      "Class 45: 1000 samples\n",
      "Class 46: 1000 samples\n",
      "Class 47: 1000 samples\n",
      "Class 48: 1000 samples\n",
      "Class 49: 1000 samples\n",
      "Class 50: 1000 samples\n",
      "Class 51: 1000 samples\n",
      "Class 52: 1000 samples\n",
      "Class 53: 1000 samples\n",
      "Class 54: 1000 samples\n",
      "Class 55: 1000 samples\n",
      "Class 56: 1000 samples\n",
      "Class 57: 1000 samples\n",
      "Class 58: 1000 samples\n",
      "Class 59: 1000 samples\n",
      "Class 60: 1000 samples\n",
      "Class 61: 1000 samples\n",
      "Class 62: 1000 samples\n",
      "Class 63: 1000 samples\n",
      "Class 64: 1000 samples\n",
      "Class 65: 1000 samples\n",
      "Class 66: 1000 samples\n",
      "Class 67: 1000 samples\n",
      "Class 68: 1000 samples\n",
      "Class 69: 1000 samples\n",
      "Class 70: 1000 samples\n",
      "Class 71: 1000 samples\n",
      "Class 72: 1000 samples\n",
      "Class 73: 1000 samples\n",
      "Class 74: 1000 samples\n",
      "Class 75: 1000 samples\n",
      "Class 76: 1000 samples\n",
      "Class 77: 1000 samples\n",
      "Class 78: 1000 samples\n",
      "Class 79: 1000 samples\n",
      "Class 80: 1000 samples\n",
      "Class 81: 1000 samples\n",
      "Class 82: 1000 samples\n",
      "Class 83: 1000 samples\n",
      "Class 84: 1000 samples\n",
      "Class 85: 1000 samples\n",
      "Class 86: 1000 samples\n",
      "Class 87: 1000 samples\n",
      "Class 88: 1000 samples\n",
      "Class 89: 1000 samples\n",
      "Class 90: 1000 samples\n",
      "Class 91: 1000 samples\n",
      "Class 92: 1000 samples\n",
      "Class 93: 1000 samples\n",
      "Class 94: 1000 samples\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout, ELU\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from keras import layers, models\n",
    "\n",
    "# ... (your existing code)\n",
    "def LoadDataNoDefCW():\n",
    "\n",
    "    print(\"Loading non-defended dataset for closed-world scenario\")\n",
    "\n",
    "    # Point to the directory storing data\n",
    "    dataset_dir = '/kaggle/input/dataset-non-defended-close-world/'\n",
    "\n",
    "    # Debug: Print dataset directory\n",
    "    print(\"Dataset directory:\", dataset_dir)\n",
    "\n",
    "    try:\n",
    "        # Load training data\n",
    "        with open(dataset_dir + 'X_train_NoDef.pkl', 'rb') as handle:\n",
    "            X_train = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"X_train loaded\")\n",
    "\n",
    "        with open(dataset_dir + 'y_train_NoDef.pkl', 'rb') as handle:\n",
    "            y_train = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"y_train loaded\")\n",
    "\n",
    "        # Load validation data\n",
    "        with open(dataset_dir + 'X_valid_NoDef.pkl', 'rb') as handle:\n",
    "            X_valid = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"X_valid loaded\")\n",
    "\n",
    "        with open(dataset_dir + 'y_valid_NoDef.pkl', 'rb') as handle:\n",
    "            y_valid = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"y_valid loaded\")\n",
    "\n",
    "        # Load testing data\n",
    "        with open(dataset_dir + 'X_test_NoDef.pkl', 'rb') as handle:\n",
    "            X_test = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"X_test loaded\")\n",
    "\n",
    "        with open(dataset_dir + 'y_test_NoDef.pkl', 'rb') as handle:\n",
    "            y_test = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"y_test loaded\")\n",
    "\n",
    "        print(\"Data dimensions:\")\n",
    "        print(\"X: Training data's shape : \", X_train.shape)\n",
    "        print(\"y: Training data's shape : \", y_train.shape)\n",
    "        print(\"X: Validation data's shape : \", X_valid.shape)\n",
    "        print(\"y: Validation data's shape : \", y_valid.shape)\n",
    "        print(\"X: Testing data's shape : \", X_test.shape)\n",
    "        print(\"y: Testing data's shape : \", y_test.shape)\n",
    "\n",
    "        # Merge datasets\n",
    "        X_all = np.concatenate((X_train, X_valid, X_test), axis=0)\n",
    "        y_all = np.concatenate((y_train, y_valid, y_test), axis=0)\n",
    "\n",
    "        print(\"Merged data dimensions:\")\n",
    "        print(\"X: Merged data's shape : \", X_all.shape)\n",
    "        print(\"y: Merged data's shape : \", y_all.shape)\n",
    "        \n",
    "        # Print features of the merged dataset\n",
    "        print(\"Features of the merged dataset:\")\n",
    "        print(X_all)\n",
    "        \n",
    "        # Check if the class distribution is balanced\n",
    "        unique_classes, class_counts = np.unique(y_all, return_counts=True)\n",
    "        class_distribution = dict(zip(unique_classes, class_counts))\n",
    "\n",
    "        print(\"Class distribution:\")\n",
    "        for class_label, count in class_distribution.items():\n",
    "            print(f\"Class {class_label}: {count} samples\")\n",
    "\n",
    "        # Plot the class distribution\n",
    "        plt.bar(class_distribution.keys(), class_distribution.values())\n",
    "        plt.xlabel('Class Label')\n",
    "        plt.ylabel('Number of Samples')\n",
    "        plt.title('Class Distribution')\n",
    "        plt.show()\n",
    "        \n",
    "        return X_all, y_all\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        return None\n",
    "\n",
    "# Call the function to load and merge data\n",
    "X_all, y_all = LoadDataNoDefCW()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9739822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T14:29:32.773479Z",
     "iopub.status.busy": "2024-01-06T14:29:32.773123Z",
     "iopub.status.idle": "2024-01-06T18:44:53.410105Z",
     "shell.execute_reply": "2024-01-06T18:44:53.408961Z"
    },
    "papermill": {
     "duration": 15320.693423,
     "end_time": "2024-01-06T18:44:53.412449",
     "exception": false,
     "start_time": "2024-01-06T14:29:32.719026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built successfully.\n",
      "Model Summary:\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv1D)       (None, 5000, 32)          288       \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 5000, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block1_adv_act1 (ELU)       (None, 5000, 32)          0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv1D)       (None, 5000, 32)          8224      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 5000, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block1_adv_act2 (ELU)       (None, 5000, 32)          0         \n",
      "                                                                 \n",
      " block1_pool (MaxPooling1D)  (None, 1250, 32)          0         \n",
      "                                                                 \n",
      " block1_dropout (Dropout)    (None, 1250, 32)          0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv1D)       (None, 1250, 64)          16448     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 1250, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block2_adv_act1 (ELU)       (None, 1250, 64)          0         \n",
      "                                                                 \n",
      " block2_conv2 (Conv1D)       (None, 1250, 64)          32832     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 1250, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block2_adv_act2 (ELU)       (None, 1250, 64)          0         \n",
      "                                                                 \n",
      " block2_pool (MaxPooling1D)  (None, 313, 64)           0         \n",
      "                                                                 \n",
      " block2_dropout (Dropout)    (None, 313, 64)           0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv1D)       (None, 313, 128)          65664     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 313, 128)          512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block3_adv_act1 (ELU)       (None, 313, 128)          0         \n",
      "                                                                 \n",
      " block3_conv2 (Conv1D)       (None, 313, 128)          131200    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 313, 128)          512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block3_adv_act2 (ELU)       (None, 313, 128)          0         \n",
      "                                                                 \n",
      " block3_pool (MaxPooling1D)  (None, 79, 128)           0         \n",
      "                                                                 \n",
      " block3_dropout (Dropout)    (None, 79, 128)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10112)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 512)               5177856   \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " fc1_act (Activation)        (None, 512)               0         \n",
      "                                                                 \n",
      " fc1_dropout (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " fc2_act (Activation)        (None, 512)               0         \n",
      "                                                                 \n",
      " fc2_dropout (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " fc3 (Dense)                 (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " fc3_act (Activation)        (None, 512)               0         \n",
      "                                                                 \n",
      " fc3_dropout (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " fc_final (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5970890 (22.78 MB)\n",
      "Trainable params: 5966922 (22.76 MB)\n",
      "Non-trainable params: 3968 (15.50 KB)\n",
      "_________________________________________________________________\n",
      "Loading non-defended dataset for closed-world scenario\n",
      "Dataset directory: /kaggle/input/dataset-non-defended-close-world/\n",
      "X_train loaded\n",
      "y_train loaded\n",
      "X_valid loaded\n",
      "y_valid loaded\n",
      "X_test loaded\n",
      "y_test loaded\n",
      "Data dimensions:\n",
      "X: Training data's shape :  (76000, 5000)\n",
      "y: Training data's shape :  (76000,)\n",
      "X: Validation data's shape :  (9500, 5000)\n",
      "y: Validation data's shape :  (9500,)\n",
      "X: Testing data's shape :  (9500, 5000)\n",
      "y: Testing data's shape :  (9500,)\n",
      "Merged data dimensions:\n",
      "X: Merged data's shape :  (95000, 5000)\n",
      "y: Merged data's shape :  (95000,)\n",
      "Features of the merged dataset:\n",
      "[[ 1. -1. -1. ... -1. -1. -1.]\n",
      " [-1.  1.  1. ...  0.  0.  0.]\n",
      " [ 1. -1. -1. ...  0.  0.  0.]\n",
      " ...\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [ 1. -1.  1. ...  0.  0.  0.]]\n",
      "Class distribution:\n",
      "Class 0: 1000 samples\n",
      "Class 1: 1000 samples\n",
      "Class 2: 1000 samples\n",
      "Class 3: 1000 samples\n",
      "Class 4: 1000 samples\n",
      "Class 5: 1000 samples\n",
      "Class 6: 1000 samples\n",
      "Class 7: 1000 samples\n",
      "Class 8: 1000 samples\n",
      "Class 9: 1000 samples\n",
      "Class 10: 1000 samples\n",
      "Class 11: 1000 samples\n",
      "Class 12: 1000 samples\n",
      "Class 13: 1000 samples\n",
      "Class 14: 1000 samples\n",
      "Class 15: 1000 samples\n",
      "Class 16: 1000 samples\n",
      "Class 17: 1000 samples\n",
      "Class 18: 1000 samples\n",
      "Class 19: 1000 samples\n",
      "Class 20: 1000 samples\n",
      "Class 21: 1000 samples\n",
      "Class 22: 1000 samples\n",
      "Class 23: 1000 samples\n",
      "Class 24: 1000 samples\n",
      "Class 25: 1000 samples\n",
      "Class 26: 1000 samples\n",
      "Class 27: 1000 samples\n",
      "Class 28: 1000 samples\n",
      "Class 29: 1000 samples\n",
      "Class 30: 1000 samples\n",
      "Class 31: 1000 samples\n",
      "Class 32: 1000 samples\n",
      "Class 33: 1000 samples\n",
      "Class 34: 1000 samples\n",
      "Class 35: 1000 samples\n",
      "Class 36: 1000 samples\n",
      "Class 37: 1000 samples\n",
      "Class 38: 1000 samples\n",
      "Class 39: 1000 samples\n",
      "Class 40: 1000 samples\n",
      "Class 41: 1000 samples\n",
      "Class 42: 1000 samples\n",
      "Class 43: 1000 samples\n",
      "Class 44: 1000 samples\n",
      "Class 45: 1000 samples\n",
      "Class 46: 1000 samples\n",
      "Class 47: 1000 samples\n",
      "Class 48: 1000 samples\n",
      "Class 49: 1000 samples\n",
      "Class 50: 1000 samples\n",
      "Class 51: 1000 samples\n",
      "Class 52: 1000 samples\n",
      "Class 53: 1000 samples\n",
      "Class 54: 1000 samples\n",
      "Class 55: 1000 samples\n",
      "Class 56: 1000 samples\n",
      "Class 57: 1000 samples\n",
      "Class 58: 1000 samples\n",
      "Class 59: 1000 samples\n",
      "Class 60: 1000 samples\n",
      "Class 61: 1000 samples\n",
      "Class 62: 1000 samples\n",
      "Class 63: 1000 samples\n",
      "Class 64: 1000 samples\n",
      "Class 65: 1000 samples\n",
      "Class 66: 1000 samples\n",
      "Class 67: 1000 samples\n",
      "Class 68: 1000 samples\n",
      "Class 69: 1000 samples\n",
      "Class 70: 1000 samples\n",
      "Class 71: 1000 samples\n",
      "Class 72: 1000 samples\n",
      "Class 73: 1000 samples\n",
      "Class 74: 1000 samples\n",
      "Class 75: 1000 samples\n",
      "Class 76: 1000 samples\n",
      "Class 77: 1000 samples\n",
      "Class 78: 1000 samples\n",
      "Class 79: 1000 samples\n",
      "Class 80: 1000 samples\n",
      "Class 81: 1000 samples\n",
      "Class 82: 1000 samples\n",
      "Class 83: 1000 samples\n",
      "Class 84: 1000 samples\n",
      "Class 85: 1000 samples\n",
      "Class 86: 1000 samples\n",
      "Class 87: 1000 samples\n",
      "Class 88: 1000 samples\n",
      "Class 89: 1000 samples\n",
      "Class 90: 1000 samples\n",
      "Class 91: 1000 samples\n",
      "Class 92: 1000 samples\n",
      "Class 93: 1000 samples\n",
      "Class 94: 1000 samples\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on Fold 1/10\n",
      "Model built successfully.\n",
      "Epoch 1/20\n",
      "1336/1336 - 97s - loss: 1.3134 - accuracy: 0.6522 - val_loss: 0.3549 - val_accuracy: 0.9086 - 97s/epoch - 73ms/step\n",
      "Epoch 2/20\n",
      "1336/1336 - 87s - loss: 0.5130 - accuracy: 0.8649 - val_loss: 0.2441 - val_accuracy: 0.9352 - 87s/epoch - 65ms/step\n",
      "Epoch 3/20\n",
      "1336/1336 - 87s - loss: 0.3827 - accuracy: 0.8997 - val_loss: 0.1885 - val_accuracy: 0.9515 - 87s/epoch - 65ms/step\n",
      "Epoch 4/20\n",
      "1336/1336 - 87s - loss: 0.3145 - accuracy: 0.9167 - val_loss: 0.2002 - val_accuracy: 0.9491 - 87s/epoch - 65ms/step\n",
      "Epoch 5/20\n",
      "1336/1336 - 87s - loss: 0.2740 - accuracy: 0.9282 - val_loss: 0.1382 - val_accuracy: 0.9646 - 87s/epoch - 65ms/step\n",
      "Epoch 6/20\n",
      "1336/1336 - 87s - loss: 0.2393 - accuracy: 0.9368 - val_loss: 0.1161 - val_accuracy: 0.9707 - 87s/epoch - 65ms/step\n",
      "Epoch 7/20\n",
      "1336/1336 - 87s - loss: 0.2161 - accuracy: 0.9435 - val_loss: 0.1118 - val_accuracy: 0.9712 - 87s/epoch - 65ms/step\n",
      "Epoch 8/20\n",
      "1336/1336 - 87s - loss: 0.1956 - accuracy: 0.9494 - val_loss: 0.1141 - val_accuracy: 0.9732 - 87s/epoch - 65ms/step\n",
      "Epoch 9/20\n",
      "1336/1336 - 87s - loss: 0.1825 - accuracy: 0.9520 - val_loss: 0.1097 - val_accuracy: 0.9737 - 87s/epoch - 65ms/step\n",
      "Epoch 10/20\n",
      "1336/1336 - 87s - loss: 0.1688 - accuracy: 0.9556 - val_loss: 0.1120 - val_accuracy: 0.9736 - 87s/epoch - 65ms/step\n",
      "Epoch 11/20\n",
      "1336/1336 - 87s - loss: 0.1623 - accuracy: 0.9571 - val_loss: 0.1074 - val_accuracy: 0.9739 - 87s/epoch - 65ms/step\n",
      "Epoch 12/20\n",
      "1336/1336 - 87s - loss: 0.1466 - accuracy: 0.9618 - val_loss: 0.1037 - val_accuracy: 0.9749 - 87s/epoch - 65ms/step\n",
      "Epoch 13/20\n",
      "1336/1336 - 87s - loss: 0.1431 - accuracy: 0.9620 - val_loss: 0.0955 - val_accuracy: 0.9776 - 87s/epoch - 65ms/step\n",
      "Epoch 14/20\n",
      "1336/1336 - 87s - loss: 0.1351 - accuracy: 0.9640 - val_loss: 0.0955 - val_accuracy: 0.9755 - 87s/epoch - 65ms/step\n",
      "Epoch 15/20\n",
      "1336/1336 - 87s - loss: 0.1300 - accuracy: 0.9652 - val_loss: 0.0893 - val_accuracy: 0.9781 - 87s/epoch - 65ms/step\n",
      "Epoch 16/20\n",
      "1336/1336 - 87s - loss: 0.1233 - accuracy: 0.9674 - val_loss: 0.0899 - val_accuracy: 0.9785 - 87s/epoch - 65ms/step\n",
      "Epoch 17/20\n",
      "1336/1336 - 87s - loss: 0.1171 - accuracy: 0.9685 - val_loss: 0.1071 - val_accuracy: 0.9748 - 87s/epoch - 65ms/step\n",
      "Epoch 18/20\n",
      "1336/1336 - 87s - loss: 0.1154 - accuracy: 0.9698 - val_loss: 0.0846 - val_accuracy: 0.9799 - 87s/epoch - 65ms/step\n",
      "Epoch 19/20\n",
      "1336/1336 - 87s - loss: 0.1121 - accuracy: 0.9702 - val_loss: 0.0962 - val_accuracy: 0.9774 - 87s/epoch - 65ms/step\n",
      "Epoch 20/20\n",
      "1336/1336 - 87s - loss: 0.1059 - accuracy: 0.9716 - val_loss: 0.0899 - val_accuracy: 0.9789 - 87s/epoch - 65ms/step\n",
      "297/297 - 3s - loss: 0.0899 - accuracy: 0.9789 - 3s/epoch - 10ms/step\n",
      "\n",
      "Fold 1 - Test Accuracy: 97.89%\n",
      "\n",
      "Training on Fold 2/10\n",
      "Model built successfully.\n",
      "Epoch 1/20\n",
      "1336/1336 - 98s - loss: 1.2120 - accuracy: 0.6816 - val_loss: 0.3573 - val_accuracy: 0.9164 - 98s/epoch - 73ms/step\n",
      "Epoch 2/20\n",
      "1336/1336 - 88s - loss: 0.5063 - accuracy: 0.8666 - val_loss: 0.2583 - val_accuracy: 0.9365 - 88s/epoch - 66ms/step\n",
      "Epoch 3/20\n",
      "1336/1336 - 88s - loss: 0.3831 - accuracy: 0.8995 - val_loss: 0.2031 - val_accuracy: 0.9512 - 88s/epoch - 66ms/step\n",
      "Epoch 4/20\n",
      "1336/1336 - 88s - loss: 0.3181 - accuracy: 0.9168 - val_loss: 0.1781 - val_accuracy: 0.9604 - 88s/epoch - 66ms/step\n",
      "Epoch 5/20\n",
      "1336/1336 - 88s - loss: 0.2712 - accuracy: 0.9284 - val_loss: 0.1706 - val_accuracy: 0.9606 - 88s/epoch - 66ms/step\n",
      "Epoch 6/20\n",
      "1336/1336 - 88s - loss: 0.2399 - accuracy: 0.9370 - val_loss: 0.1489 - val_accuracy: 0.9645 - 88s/epoch - 66ms/step\n",
      "Epoch 7/20\n",
      "1336/1336 - 88s - loss: 0.2138 - accuracy: 0.9430 - val_loss: 0.1484 - val_accuracy: 0.9660 - 88s/epoch - 66ms/step\n",
      "Epoch 8/20\n",
      "1336/1336 - 88s - loss: 0.1985 - accuracy: 0.9477 - val_loss: 0.1391 - val_accuracy: 0.9675 - 88s/epoch - 66ms/step\n",
      "Epoch 9/20\n",
      "1336/1336 - 88s - loss: 0.1821 - accuracy: 0.9520 - val_loss: 0.1205 - val_accuracy: 0.9725 - 88s/epoch - 66ms/step\n",
      "Epoch 10/20\n",
      "1336/1336 - 88s - loss: 0.1681 - accuracy: 0.9552 - val_loss: 0.1226 - val_accuracy: 0.9698 - 88s/epoch - 66ms/step\n",
      "Epoch 11/20\n",
      "1336/1336 - 88s - loss: 0.1623 - accuracy: 0.9574 - val_loss: 0.1206 - val_accuracy: 0.9709 - 88s/epoch - 66ms/step\n",
      "Epoch 12/20\n",
      "1336/1336 - 88s - loss: 0.1455 - accuracy: 0.9608 - val_loss: 0.1136 - val_accuracy: 0.9745 - 88s/epoch - 66ms/step\n",
      "Epoch 13/20\n",
      "1336/1336 - 88s - loss: 0.1395 - accuracy: 0.9624 - val_loss: 0.1183 - val_accuracy: 0.9716 - 88s/epoch - 66ms/step\n",
      "Epoch 14/20\n",
      "1336/1336 - 88s - loss: 0.1351 - accuracy: 0.9641 - val_loss: 0.1165 - val_accuracy: 0.9733 - 88s/epoch - 66ms/step\n",
      "Epoch 15/20\n",
      "1336/1336 - 88s - loss: 0.1275 - accuracy: 0.9663 - val_loss: 0.1115 - val_accuracy: 0.9745 - 88s/epoch - 66ms/step\n",
      "Epoch 16/20\n",
      "1336/1336 - 88s - loss: 0.1189 - accuracy: 0.9682 - val_loss: 0.1170 - val_accuracy: 0.9748 - 88s/epoch - 66ms/step\n",
      "Epoch 17/20\n",
      "1336/1336 - 88s - loss: 0.1178 - accuracy: 0.9680 - val_loss: 0.1138 - val_accuracy: 0.9745 - 88s/epoch - 66ms/step\n",
      "Epoch 18/20\n",
      "1336/1336 - 88s - loss: 0.1125 - accuracy: 0.9698 - val_loss: 0.1121 - val_accuracy: 0.9756 - 88s/epoch - 66ms/step\n",
      "297/297 - 3s - loss: 0.1115 - accuracy: 0.9745 - 3s/epoch - 10ms/step\n",
      "\n",
      "Fold 2 - Test Accuracy: 97.45%\n",
      "\n",
      "Training on Fold 3/10\n",
      "Model built successfully.\n",
      "Epoch 1/20\n",
      "1336/1336 - 96s - loss: 1.3321 - accuracy: 0.6483 - val_loss: 0.3492 - val_accuracy: 0.9083 - 96s/epoch - 72ms/step\n",
      "Epoch 2/20\n",
      "1336/1336 - 88s - loss: 0.5224 - accuracy: 0.8637 - val_loss: 0.2266 - val_accuracy: 0.9436 - 88s/epoch - 66ms/step\n",
      "Epoch 3/20\n",
      "1336/1336 - 88s - loss: 0.3853 - accuracy: 0.8988 - val_loss: 0.1698 - val_accuracy: 0.9553 - 88s/epoch - 66ms/step\n",
      "Epoch 4/20\n",
      "1336/1336 - 87s - loss: 0.3097 - accuracy: 0.9178 - val_loss: 0.1383 - val_accuracy: 0.9654 - 87s/epoch - 65ms/step\n",
      "Epoch 5/20\n",
      "1336/1336 - 87s - loss: 0.2751 - accuracy: 0.9273 - val_loss: 0.1306 - val_accuracy: 0.9676 - 87s/epoch - 65ms/step\n",
      "Epoch 6/20\n",
      "1336/1336 - 87s - loss: 0.2425 - accuracy: 0.9358 - val_loss: 0.1167 - val_accuracy: 0.9714 - 87s/epoch - 65ms/step\n",
      "Epoch 7/20\n",
      "1336/1336 - 87s - loss: 0.2162 - accuracy: 0.9428 - val_loss: 0.1247 - val_accuracy: 0.9707 - 87s/epoch - 65ms/step\n",
      "Epoch 8/20\n",
      "1336/1336 - 87s - loss: 0.2008 - accuracy: 0.9472 - val_loss: 0.1100 - val_accuracy: 0.9753 - 87s/epoch - 65ms/step\n",
      "Epoch 9/20\n",
      "1336/1336 - 87s - loss: 0.1856 - accuracy: 0.9513 - val_loss: 0.1121 - val_accuracy: 0.9716 - 87s/epoch - 65ms/step\n",
      "Epoch 10/20\n",
      "1336/1336 - 88s - loss: 0.1746 - accuracy: 0.9539 - val_loss: 0.1050 - val_accuracy: 0.9735 - 88s/epoch - 65ms/step\n",
      "Epoch 11/20\n",
      "1336/1336 - 87s - loss: 0.1599 - accuracy: 0.9576 - val_loss: 0.0910 - val_accuracy: 0.9788 - 87s/epoch - 65ms/step\n",
      "Epoch 12/20\n",
      "1336/1336 - 87s - loss: 0.1564 - accuracy: 0.9586 - val_loss: 0.0992 - val_accuracy: 0.9767 - 87s/epoch - 65ms/step\n",
      "Epoch 13/20\n",
      "1336/1336 - 87s - loss: 0.1465 - accuracy: 0.9610 - val_loss: 0.0910 - val_accuracy: 0.9786 - 87s/epoch - 65ms/step\n",
      "Epoch 14/20\n",
      "1336/1336 - 87s - loss: 0.1372 - accuracy: 0.9634 - val_loss: 0.0928 - val_accuracy: 0.9785 - 87s/epoch - 65ms/step\n",
      "Epoch 15/20\n",
      "1336/1336 - 87s - loss: 0.1326 - accuracy: 0.9645 - val_loss: 0.0929 - val_accuracy: 0.9777 - 87s/epoch - 65ms/step\n",
      "Epoch 16/20\n",
      "1336/1336 - 87s - loss: 0.1236 - accuracy: 0.9673 - val_loss: 0.0895 - val_accuracy: 0.9789 - 87s/epoch - 65ms/step\n",
      "Epoch 17/20\n",
      "1336/1336 - 87s - loss: 0.1239 - accuracy: 0.9670 - val_loss: 0.0887 - val_accuracy: 0.9788 - 87s/epoch - 65ms/step\n",
      "Epoch 18/20\n",
      "1336/1336 - 87s - loss: 0.1169 - accuracy: 0.9696 - val_loss: 0.0817 - val_accuracy: 0.9809 - 87s/epoch - 65ms/step\n",
      "Epoch 19/20\n",
      "1336/1336 - 87s - loss: 0.1105 - accuracy: 0.9706 - val_loss: 0.0911 - val_accuracy: 0.9793 - 87s/epoch - 65ms/step\n",
      "Epoch 20/20\n",
      "1336/1336 - 87s - loss: 0.1085 - accuracy: 0.9712 - val_loss: 0.0788 - val_accuracy: 0.9814 - 87s/epoch - 65ms/step\n",
      "297/297 - 3s - loss: 0.0788 - accuracy: 0.9814 - 3s/epoch - 10ms/step\n",
      "\n",
      "Fold 3 - Test Accuracy: 98.14%\n",
      "\n",
      "Training on Fold 4/10\n",
      "Model built successfully.\n",
      "Epoch 1/20\n",
      "1336/1336 - 96s - loss: 1.2566 - accuracy: 0.6704 - val_loss: 0.3421 - val_accuracy: 0.9109 - 96s/epoch - 72ms/step\n",
      "Epoch 2/20\n",
      "1336/1336 - 87s - loss: 0.5020 - accuracy: 0.8680 - val_loss: 0.2440 - val_accuracy: 0.9378 - 87s/epoch - 65ms/step\n",
      "Epoch 3/20\n",
      "1336/1336 - 86s - loss: 0.3674 - accuracy: 0.9025 - val_loss: 0.1882 - val_accuracy: 0.9527 - 86s/epoch - 64ms/step\n",
      "Epoch 4/20\n",
      "1336/1336 - 86s - loss: 0.3132 - accuracy: 0.9183 - val_loss: 0.1590 - val_accuracy: 0.9608 - 86s/epoch - 65ms/step\n",
      "Epoch 5/20\n",
      "1336/1336 - 86s - loss: 0.2656 - accuracy: 0.9302 - val_loss: 0.1717 - val_accuracy: 0.9580 - 86s/epoch - 65ms/step\n",
      "Epoch 6/20\n",
      "1336/1336 - 86s - loss: 0.2363 - accuracy: 0.9381 - val_loss: 0.1440 - val_accuracy: 0.9636 - 86s/epoch - 65ms/step\n",
      "Epoch 7/20\n",
      "1336/1336 - 86s - loss: 0.2115 - accuracy: 0.9453 - val_loss: 0.1384 - val_accuracy: 0.9644 - 86s/epoch - 65ms/step\n",
      "Epoch 8/20\n",
      "1336/1336 - 86s - loss: 0.1984 - accuracy: 0.9484 - val_loss: 0.1259 - val_accuracy: 0.9682 - 86s/epoch - 64ms/step\n",
      "Epoch 9/20\n",
      "1336/1336 - 86s - loss: 0.1788 - accuracy: 0.9531 - val_loss: 0.1210 - val_accuracy: 0.9707 - 86s/epoch - 65ms/step\n",
      "Epoch 10/20\n",
      "1336/1336 - 86s - loss: 0.1679 - accuracy: 0.9560 - val_loss: 0.1234 - val_accuracy: 0.9705 - 86s/epoch - 65ms/step\n",
      "Epoch 11/20\n",
      "1336/1336 - 86s - loss: 0.1525 - accuracy: 0.9601 - val_loss: 0.1122 - val_accuracy: 0.9723 - 86s/epoch - 64ms/step\n",
      "Epoch 12/20\n",
      "1336/1336 - 86s - loss: 0.1490 - accuracy: 0.9607 - val_loss: 0.1134 - val_accuracy: 0.9693 - 86s/epoch - 64ms/step\n",
      "Epoch 13/20\n",
      "1336/1336 - 86s - loss: 0.1410 - accuracy: 0.9629 - val_loss: 0.1121 - val_accuracy: 0.9724 - 86s/epoch - 64ms/step\n",
      "Epoch 14/20\n",
      "1336/1336 - 86s - loss: 0.1314 - accuracy: 0.9651 - val_loss: 0.1164 - val_accuracy: 0.9734 - 86s/epoch - 64ms/step\n",
      "Epoch 15/20\n",
      "1336/1336 - 86s - loss: 0.1270 - accuracy: 0.9661 - val_loss: 0.1014 - val_accuracy: 0.9748 - 86s/epoch - 64ms/step\n",
      "Epoch 16/20\n",
      "1336/1336 - 86s - loss: 0.1204 - accuracy: 0.9680 - val_loss: 0.1012 - val_accuracy: 0.9753 - 86s/epoch - 65ms/step\n",
      "Epoch 17/20\n",
      "1336/1336 - 86s - loss: 0.1173 - accuracy: 0.9688 - val_loss: 0.1194 - val_accuracy: 0.9740 - 86s/epoch - 65ms/step\n",
      "Epoch 18/20\n",
      "1336/1336 - 86s - loss: 0.1141 - accuracy: 0.9700 - val_loss: 0.1052 - val_accuracy: 0.9757 - 86s/epoch - 65ms/step\n",
      "Epoch 19/20\n",
      "1336/1336 - 86s - loss: 0.1092 - accuracy: 0.9704 - val_loss: 0.0976 - val_accuracy: 0.9764 - 86s/epoch - 64ms/step\n",
      "Epoch 20/20\n",
      "1336/1336 - 86s - loss: 0.1041 - accuracy: 0.9719 - val_loss: 0.1036 - val_accuracy: 0.9776 - 86s/epoch - 64ms/step\n",
      "297/297 - 3s - loss: 0.1036 - accuracy: 0.9776 - 3s/epoch - 9ms/step\n",
      "\n",
      "Fold 4 - Test Accuracy: 97.76%\n",
      "\n",
      "Training on Fold 5/10\n",
      "Model built successfully.\n",
      "Epoch 1/20\n",
      "1336/1336 - 95s - loss: 1.2903 - accuracy: 0.6616 - val_loss: 0.3822 - val_accuracy: 0.9023 - 95s/epoch - 71ms/step\n",
      "Epoch 2/20\n",
      "1336/1336 - 86s - loss: 0.5206 - accuracy: 0.8635 - val_loss: 0.2398 - val_accuracy: 0.9379 - 86s/epoch - 65ms/step\n",
      "Epoch 3/20\n",
      "1336/1336 - 86s - loss: 0.3869 - accuracy: 0.8984 - val_loss: 0.2089 - val_accuracy: 0.9455 - 86s/epoch - 64ms/step\n",
      "Epoch 4/20\n",
      "1336/1336 - 86s - loss: 0.3229 - accuracy: 0.9157 - val_loss: 0.1904 - val_accuracy: 0.9533 - 86s/epoch - 65ms/step\n",
      "Epoch 5/20\n",
      "1336/1336 - 86s - loss: 0.2747 - accuracy: 0.9277 - val_loss: 0.1558 - val_accuracy: 0.9609 - 86s/epoch - 65ms/step\n",
      "Epoch 6/20\n",
      "1336/1336 - 86s - loss: 0.2406 - accuracy: 0.9356 - val_loss: 0.1468 - val_accuracy: 0.9641 - 86s/epoch - 64ms/step\n",
      "Epoch 7/20\n",
      "1336/1336 - 86s - loss: 0.2257 - accuracy: 0.9410 - val_loss: 0.1362 - val_accuracy: 0.9675 - 86s/epoch - 64ms/step\n",
      "Epoch 8/20\n",
      "1336/1336 - 86s - loss: 0.2022 - accuracy: 0.9474 - val_loss: 0.1218 - val_accuracy: 0.9703 - 86s/epoch - 64ms/step\n",
      "Epoch 9/20\n",
      "1336/1336 - 86s - loss: 0.1852 - accuracy: 0.9508 - val_loss: 0.1294 - val_accuracy: 0.9689 - 86s/epoch - 64ms/step\n",
      "Epoch 10/20\n",
      "1336/1336 - 86s - loss: 0.1740 - accuracy: 0.9550 - val_loss: 0.1207 - val_accuracy: 0.9699 - 86s/epoch - 65ms/step\n",
      "Epoch 11/20\n",
      "1336/1336 - 86s - loss: 0.1618 - accuracy: 0.9578 - val_loss: 0.1077 - val_accuracy: 0.9731 - 86s/epoch - 65ms/step\n",
      "Epoch 12/20\n",
      "1336/1336 - 86s - loss: 0.1516 - accuracy: 0.9591 - val_loss: 0.1114 - val_accuracy: 0.9723 - 86s/epoch - 64ms/step\n",
      "Epoch 13/20\n",
      "1336/1336 - 86s - loss: 0.1425 - accuracy: 0.9625 - val_loss: 0.1074 - val_accuracy: 0.9739 - 86s/epoch - 64ms/step\n",
      "Epoch 14/20\n",
      "1336/1336 - 86s - loss: 0.1354 - accuracy: 0.9633 - val_loss: 0.1110 - val_accuracy: 0.9726 - 86s/epoch - 64ms/step\n",
      "Epoch 15/20\n",
      "1336/1336 - 86s - loss: 0.1338 - accuracy: 0.9648 - val_loss: 0.1131 - val_accuracy: 0.9728 - 86s/epoch - 64ms/step\n",
      "Epoch 16/20\n",
      "1336/1336 - 86s - loss: 0.1256 - accuracy: 0.9665 - val_loss: 0.1049 - val_accuracy: 0.9735 - 86s/epoch - 64ms/step\n",
      "Epoch 17/20\n",
      "1336/1336 - 86s - loss: 0.1196 - accuracy: 0.9680 - val_loss: 0.1064 - val_accuracy: 0.9741 - 86s/epoch - 64ms/step\n",
      "Epoch 18/20\n",
      "1336/1336 - 86s - loss: 0.1162 - accuracy: 0.9690 - val_loss: 0.1030 - val_accuracy: 0.9752 - 86s/epoch - 64ms/step\n",
      "Epoch 19/20\n",
      "1336/1336 - 86s - loss: 0.1123 - accuracy: 0.9700 - val_loss: 0.0947 - val_accuracy: 0.9778 - 86s/epoch - 64ms/step\n",
      "Epoch 20/20\n",
      "1336/1336 - 86s - loss: 0.1068 - accuracy: 0.9715 - val_loss: 0.0980 - val_accuracy: 0.9777 - 86s/epoch - 64ms/step\n",
      "297/297 - 3s - loss: 0.0980 - accuracy: 0.9777 - 3s/epoch - 9ms/step\n",
      "\n",
      "Fold 5 - Test Accuracy: 97.77%\n",
      "\n",
      "Training on Fold 6/10\n",
      "Model built successfully.\n",
      "Epoch 1/20\n",
      "1336/1336 - 94s - loss: 1.2790 - accuracy: 0.6641 - val_loss: 0.4050 - val_accuracy: 0.9017 - 94s/epoch - 70ms/step\n",
      "Epoch 2/20\n",
      "1336/1336 - 86s - loss: 0.5064 - accuracy: 0.8683 - val_loss: 0.2414 - val_accuracy: 0.9404 - 86s/epoch - 64ms/step\n",
      "Epoch 3/20\n",
      "1336/1336 - 86s - loss: 0.3812 - accuracy: 0.9007 - val_loss: 0.1969 - val_accuracy: 0.9521 - 86s/epoch - 64ms/step\n",
      "Epoch 4/20\n",
      "1336/1336 - 86s - loss: 0.3150 - accuracy: 0.9171 - val_loss: 0.1895 - val_accuracy: 0.9528 - 86s/epoch - 64ms/step\n",
      "Epoch 5/20\n",
      "1336/1336 - 86s - loss: 0.2750 - accuracy: 0.9279 - val_loss: 0.1430 - val_accuracy: 0.9654 - 86s/epoch - 64ms/step\n",
      "Epoch 6/20\n",
      "1336/1336 - 86s - loss: 0.2430 - accuracy: 0.9353 - val_loss: 0.1584 - val_accuracy: 0.9615 - 86s/epoch - 64ms/step\n",
      "Epoch 7/20\n",
      "1336/1336 - 86s - loss: 0.2112 - accuracy: 0.9444 - val_loss: 0.1440 - val_accuracy: 0.9651 - 86s/epoch - 64ms/step\n",
      "Epoch 8/20\n",
      "1336/1336 - 86s - loss: 0.1996 - accuracy: 0.9472 - val_loss: 0.1522 - val_accuracy: 0.9638 - 86s/epoch - 64ms/step\n",
      "297/297 - 3s - loss: 0.1430 - accuracy: 0.9654 - 3s/epoch - 9ms/step\n",
      "\n",
      "Fold 6 - Test Accuracy: 96.54%\n",
      "\n",
      "Training on Fold 7/10\n",
      "Model built successfully.\n",
      "Epoch 1/20\n",
      "1336/1336 - 94s - loss: 1.2555 - accuracy: 0.6696 - val_loss: 0.3532 - val_accuracy: 0.9076 - 94s/epoch - 71ms/step\n",
      "Epoch 2/20\n",
      "1336/1336 - 86s - loss: 0.5034 - accuracy: 0.8675 - val_loss: 0.2327 - val_accuracy: 0.9409 - 86s/epoch - 65ms/step\n",
      "Epoch 3/20\n",
      "1336/1336 - 86s - loss: 0.3739 - accuracy: 0.9019 - val_loss: 0.2143 - val_accuracy: 0.9491 - 86s/epoch - 64ms/step\n",
      "Epoch 4/20\n",
      "1336/1336 - 86s - loss: 0.3083 - accuracy: 0.9195 - val_loss: 0.1561 - val_accuracy: 0.9609 - 86s/epoch - 65ms/step\n",
      "Epoch 5/20\n",
      "1336/1336 - 86s - loss: 0.2690 - accuracy: 0.9288 - val_loss: 0.1492 - val_accuracy: 0.9623 - 86s/epoch - 65ms/step\n",
      "Epoch 6/20\n",
      "1336/1336 - 86s - loss: 0.2411 - accuracy: 0.9370 - val_loss: 0.1454 - val_accuracy: 0.9647 - 86s/epoch - 64ms/step\n",
      "Epoch 7/20\n",
      "1336/1336 - 86s - loss: 0.2165 - accuracy: 0.9427 - val_loss: 0.1282 - val_accuracy: 0.9682 - 86s/epoch - 64ms/step\n",
      "Epoch 8/20\n",
      "1336/1336 - 86s - loss: 0.1971 - accuracy: 0.9482 - val_loss: 0.1159 - val_accuracy: 0.9691 - 86s/epoch - 64ms/step\n",
      "Epoch 9/20\n",
      "1336/1336 - 86s - loss: 0.1820 - accuracy: 0.9524 - val_loss: 0.1252 - val_accuracy: 0.9704 - 86s/epoch - 64ms/step\n",
      "Epoch 10/20\n",
      "1336/1336 - 86s - loss: 0.1704 - accuracy: 0.9555 - val_loss: 0.1064 - val_accuracy: 0.9732 - 86s/epoch - 64ms/step\n",
      "Epoch 11/20\n",
      "1336/1336 - 86s - loss: 0.1567 - accuracy: 0.9595 - val_loss: 0.1088 - val_accuracy: 0.9737 - 86s/epoch - 64ms/step\n",
      "Epoch 12/20\n",
      "1336/1336 - 86s - loss: 0.1505 - accuracy: 0.9603 - val_loss: 0.1101 - val_accuracy: 0.9731 - 86s/epoch - 65ms/step\n",
      "Epoch 13/20\n",
      "1336/1336 - 87s - loss: 0.1426 - accuracy: 0.9629 - val_loss: 0.1032 - val_accuracy: 0.9749 - 87s/epoch - 65ms/step\n",
      "Epoch 14/20\n",
      "1336/1336 - 87s - loss: 0.1379 - accuracy: 0.9641 - val_loss: 0.1126 - val_accuracy: 0.9729 - 87s/epoch - 65ms/step\n",
      "Epoch 15/20\n",
      "1336/1336 - 87s - loss: 0.1284 - accuracy: 0.9658 - val_loss: 0.1073 - val_accuracy: 0.9744 - 87s/epoch - 65ms/step\n",
      "Epoch 16/20\n",
      "1336/1336 - 87s - loss: 0.1244 - accuracy: 0.9671 - val_loss: 0.0986 - val_accuracy: 0.9766 - 87s/epoch - 65ms/step\n",
      "Epoch 17/20\n",
      "1336/1336 - 86s - loss: 0.1189 - accuracy: 0.9683 - val_loss: 0.0940 - val_accuracy: 0.9757 - 86s/epoch - 65ms/step\n",
      "Epoch 18/20\n",
      "1336/1336 - 86s - loss: 0.1113 - accuracy: 0.9705 - val_loss: 0.0896 - val_accuracy: 0.9780 - 86s/epoch - 65ms/step\n",
      "Epoch 19/20\n",
      "1336/1336 - 86s - loss: 0.1122 - accuracy: 0.9697 - val_loss: 0.0938 - val_accuracy: 0.9771 - 86s/epoch - 65ms/step\n",
      "Epoch 20/20\n",
      "1336/1336 - 86s - loss: 0.1044 - accuracy: 0.9724 - val_loss: 0.0955 - val_accuracy: 0.9761 - 86s/epoch - 65ms/step\n",
      "297/297 - 3s - loss: 0.0955 - accuracy: 0.9761 - 3s/epoch - 9ms/step\n",
      "\n",
      "Fold 7 - Test Accuracy: 97.61%\n",
      "\n",
      "Training on Fold 8/10\n",
      "Model built successfully.\n",
      "Epoch 1/20\n",
      "1336/1336 - 94s - loss: 1.2826 - accuracy: 0.6619 - val_loss: 0.3471 - val_accuracy: 0.9118 - 94s/epoch - 71ms/step\n",
      "Epoch 2/20\n",
      "1336/1336 - 86s - loss: 0.5155 - accuracy: 0.8653 - val_loss: 0.2218 - val_accuracy: 0.9427 - 86s/epoch - 65ms/step\n",
      "Epoch 3/20\n",
      "1336/1336 - 86s - loss: 0.3837 - accuracy: 0.8991 - val_loss: 0.1875 - val_accuracy: 0.9508 - 86s/epoch - 65ms/step\n",
      "Epoch 4/20\n",
      "1336/1336 - 86s - loss: 0.3200 - accuracy: 0.9161 - val_loss: 0.1610 - val_accuracy: 0.9567 - 86s/epoch - 65ms/step\n",
      "Epoch 5/20\n",
      "1336/1336 - 86s - loss: 0.2773 - accuracy: 0.9278 - val_loss: 0.1477 - val_accuracy: 0.9627 - 86s/epoch - 64ms/step\n",
      "Epoch 6/20\n",
      "1336/1336 - 86s - loss: 0.2415 - accuracy: 0.9361 - val_loss: 0.1373 - val_accuracy: 0.9658 - 86s/epoch - 64ms/step\n",
      "Epoch 7/20\n",
      "1336/1336 - 86s - loss: 0.2196 - accuracy: 0.9413 - val_loss: 0.1174 - val_accuracy: 0.9707 - 86s/epoch - 65ms/step\n",
      "Epoch 8/20\n",
      "1336/1336 - 86s - loss: 0.1921 - accuracy: 0.9494 - val_loss: 0.1148 - val_accuracy: 0.9701 - 86s/epoch - 65ms/step\n",
      "Epoch 9/20\n",
      "1336/1336 - 86s - loss: 0.1848 - accuracy: 0.9512 - val_loss: 0.1155 - val_accuracy: 0.9701 - 86s/epoch - 65ms/step\n",
      "Epoch 10/20\n",
      "1336/1336 - 86s - loss: 0.1748 - accuracy: 0.9545 - val_loss: 0.1018 - val_accuracy: 0.9738 - 86s/epoch - 65ms/step\n",
      "Epoch 11/20\n",
      "1336/1336 - 86s - loss: 0.1589 - accuracy: 0.9583 - val_loss: 0.1083 - val_accuracy: 0.9718 - 86s/epoch - 65ms/step\n",
      "Epoch 12/20\n",
      "1336/1336 - 86s - loss: 0.1497 - accuracy: 0.9605 - val_loss: 0.1025 - val_accuracy: 0.9748 - 86s/epoch - 65ms/step\n",
      "Epoch 13/20\n",
      "1336/1336 - 86s - loss: 0.1426 - accuracy: 0.9618 - val_loss: 0.1106 - val_accuracy: 0.9715 - 86s/epoch - 64ms/step\n",
      "297/297 - 3s - loss: 0.1018 - accuracy: 0.9738 - 3s/epoch - 9ms/step\n",
      "\n",
      "Fold 8 - Test Accuracy: 97.38%\n",
      "\n",
      "Training on Fold 9/10\n",
      "Model built successfully.\n",
      "Epoch 1/20\n",
      "1336/1336 - 95s - loss: 1.2724 - accuracy: 0.6660 - val_loss: 0.4013 - val_accuracy: 0.8934 - 95s/epoch - 71ms/step\n",
      "Epoch 2/20\n",
      "1336/1336 - 87s - loss: 0.5087 - accuracy: 0.8672 - val_loss: 0.2366 - val_accuracy: 0.9389 - 87s/epoch - 65ms/step\n",
      "Epoch 3/20\n",
      "1336/1336 - 86s - loss: 0.3806 - accuracy: 0.9009 - val_loss: 0.1728 - val_accuracy: 0.9579 - 86s/epoch - 65ms/step\n",
      "Epoch 4/20\n",
      "1336/1336 - 86s - loss: 0.3143 - accuracy: 0.9176 - val_loss: 0.1586 - val_accuracy: 0.9619 - 86s/epoch - 65ms/step\n",
      "Epoch 5/20\n",
      "1336/1336 - 86s - loss: 0.2739 - accuracy: 0.9282 - val_loss: 0.1282 - val_accuracy: 0.9669 - 86s/epoch - 65ms/step\n",
      "Epoch 6/20\n",
      "1336/1336 - 86s - loss: 0.2419 - accuracy: 0.9369 - val_loss: 0.1226 - val_accuracy: 0.9697 - 86s/epoch - 65ms/step\n",
      "Epoch 7/20\n",
      "1336/1336 - 86s - loss: 0.2177 - accuracy: 0.9429 - val_loss: 0.1197 - val_accuracy: 0.9713 - 86s/epoch - 65ms/step\n",
      "Epoch 8/20\n",
      "1336/1336 - 86s - loss: 0.1978 - accuracy: 0.9476 - val_loss: 0.1137 - val_accuracy: 0.9711 - 86s/epoch - 65ms/step\n",
      "Epoch 9/20\n",
      "1336/1336 - 86s - loss: 0.1828 - accuracy: 0.9519 - val_loss: 0.1007 - val_accuracy: 0.9760 - 86s/epoch - 65ms/step\n",
      "Epoch 10/20\n",
      "1336/1336 - 86s - loss: 0.1724 - accuracy: 0.9540 - val_loss: 0.0947 - val_accuracy: 0.9765 - 86s/epoch - 65ms/step\n",
      "Epoch 11/20\n",
      "1336/1336 - 86s - loss: 0.1574 - accuracy: 0.9584 - val_loss: 0.0910 - val_accuracy: 0.9779 - 86s/epoch - 65ms/step\n",
      "Epoch 12/20\n",
      "1336/1336 - 86s - loss: 0.1541 - accuracy: 0.9595 - val_loss: 0.0963 - val_accuracy: 0.9771 - 86s/epoch - 65ms/step\n",
      "Epoch 13/20\n",
      "1336/1336 - 86s - loss: 0.1457 - accuracy: 0.9616 - val_loss: 0.1031 - val_accuracy: 0.9751 - 86s/epoch - 65ms/step\n",
      "Epoch 14/20\n",
      "1336/1336 - 86s - loss: 0.1368 - accuracy: 0.9641 - val_loss: 0.0938 - val_accuracy: 0.9765 - 86s/epoch - 65ms/step\n",
      "297/297 - 3s - loss: 0.0910 - accuracy: 0.9779 - 3s/epoch - 9ms/step\n",
      "\n",
      "Fold 9 - Test Accuracy: 97.79%\n",
      "\n",
      "Training on Fold 10/10\n",
      "Model built successfully.\n",
      "Epoch 1/20\n",
      "1336/1336 - 95s - loss: 1.2119 - accuracy: 0.6804 - val_loss: 0.3377 - val_accuracy: 0.9179 - 95s/epoch - 71ms/step\n",
      "Epoch 2/20\n",
      "1336/1336 - 86s - loss: 0.4924 - accuracy: 0.8712 - val_loss: 0.2259 - val_accuracy: 0.9446 - 86s/epoch - 65ms/step\n",
      "Epoch 3/20\n",
      "1336/1336 - 86s - loss: 0.3700 - accuracy: 0.9025 - val_loss: 0.1957 - val_accuracy: 0.9516 - 86s/epoch - 65ms/step\n",
      "Epoch 4/20\n",
      "1336/1336 - 86s - loss: 0.3024 - accuracy: 0.9207 - val_loss: 0.1587 - val_accuracy: 0.9638 - 86s/epoch - 65ms/step\n",
      "Epoch 5/20\n",
      "1336/1336 - 86s - loss: 0.2666 - accuracy: 0.9300 - val_loss: 0.1393 - val_accuracy: 0.9679 - 86s/epoch - 65ms/step\n",
      "Epoch 6/20\n",
      "1336/1336 - 86s - loss: 0.2330 - accuracy: 0.9387 - val_loss: 0.1395 - val_accuracy: 0.9701 - 86s/epoch - 64ms/step\n",
      "Epoch 7/20\n",
      "1336/1336 - 86s - loss: 0.2098 - accuracy: 0.9447 - val_loss: 0.1215 - val_accuracy: 0.9711 - 86s/epoch - 64ms/step\n",
      "Epoch 8/20\n",
      "1336/1336 - 86s - loss: 0.1941 - accuracy: 0.9481 - val_loss: 0.1262 - val_accuracy: 0.9716 - 86s/epoch - 64ms/step\n",
      "Epoch 9/20\n",
      "1336/1336 - 86s - loss: 0.1785 - accuracy: 0.9527 - val_loss: 0.1116 - val_accuracy: 0.9737 - 86s/epoch - 64ms/step\n",
      "Epoch 10/20\n",
      "1336/1336 - 86s - loss: 0.1689 - accuracy: 0.9553 - val_loss: 0.1253 - val_accuracy: 0.9706 - 86s/epoch - 64ms/step\n",
      "Epoch 11/20\n",
      "1336/1336 - 86s - loss: 0.1582 - accuracy: 0.9593 - val_loss: 0.1159 - val_accuracy: 0.9720 - 86s/epoch - 64ms/step\n",
      "Epoch 12/20\n",
      "1336/1336 - 86s - loss: 0.1452 - accuracy: 0.9614 - val_loss: 0.1087 - val_accuracy: 0.9748 - 86s/epoch - 64ms/step\n",
      "Epoch 13/20\n",
      "1336/1336 - 86s - loss: 0.1400 - accuracy: 0.9632 - val_loss: 0.1031 - val_accuracy: 0.9760 - 86s/epoch - 64ms/step\n",
      "Epoch 14/20\n",
      "1336/1336 - 86s - loss: 0.1327 - accuracy: 0.9648 - val_loss: 0.1020 - val_accuracy: 0.9772 - 86s/epoch - 65ms/step\n",
      "Epoch 15/20\n",
      "1336/1336 - 86s - loss: 0.1264 - accuracy: 0.9667 - val_loss: 0.1042 - val_accuracy: 0.9758 - 86s/epoch - 65ms/step\n",
      "Epoch 16/20\n",
      "1336/1336 - 86s - loss: 0.1188 - accuracy: 0.9678 - val_loss: 0.1091 - val_accuracy: 0.9774 - 86s/epoch - 65ms/step\n",
      "Epoch 17/20\n",
      "1336/1336 - 86s - loss: 0.1174 - accuracy: 0.9684 - val_loss: 0.0977 - val_accuracy: 0.9786 - 86s/epoch - 65ms/step\n",
      "Epoch 18/20\n",
      "1336/1336 - 87s - loss: 0.1107 - accuracy: 0.9708 - val_loss: 0.0977 - val_accuracy: 0.9773 - 87s/epoch - 65ms/step\n",
      "Epoch 19/20\n",
      "1336/1336 - 86s - loss: 0.1094 - accuracy: 0.9711 - val_loss: 0.0994 - val_accuracy: 0.9779 - 86s/epoch - 65ms/step\n",
      "Epoch 20/20\n",
      "1336/1336 - 86s - loss: 0.1025 - accuracy: 0.9725 - val_loss: 0.0975 - val_accuracy: 0.9775 - 86s/epoch - 65ms/step\n",
      "297/297 - 3s - loss: 0.0975 - accuracy: 0.9775 - 3s/epoch - 9ms/step\n",
      "\n",
      "Fold 10 - Test Accuracy: 97.75%\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# ... (previous imports and code remain unchanged)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout, ELU\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DFNet:\n",
    "    @staticmethod\n",
    "    def build(input_shape, classes, dropout_rate1=0.7, dropout_rate2=0.5, dropout_rate_fc=0.5):\n",
    "        model = Sequential()\n",
    "\n",
    "        # ... (previous model definition remains unchanged)\n",
    "        # Block 1\n",
    "        filter_num = [None, 32, 64, 128, 256]\n",
    "        kernel_size = [None, 8, 8, 8, 8]\n",
    "        conv_stride_size = [None, 1, 1, 1, 1]\n",
    "        pool_stride_size = [None, 4, 4, 4, 4]\n",
    "        pool_size = [None, 8, 8, 8, 8]\n",
    "\n",
    "        for i in range(1, 4):\n",
    "            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n",
    "                             strides=conv_stride_size[i], padding='same',\n",
    "                             input_shape=input_shape if i == 1 else (None, input_shape[1]),\n",
    "                             name=f'block{i}_conv1'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i * 2 - 1}'))\n",
    "            model.add(ELU(alpha=1.0, name=f'block{i}_adv_act1'))\n",
    "\n",
    "            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n",
    "                             strides=conv_stride_size[i], padding='same',\n",
    "                             name=f'block{i}_conv2'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i * 2}'))\n",
    "            model.add(ELU(alpha=1.0, name=f'block{i}_adv_act2'))\n",
    "\n",
    "            model.add(MaxPooling1D(pool_size=pool_size[i], strides=pool_stride_size[i],\n",
    "                                   padding='same', name=f'block{i}_pool'))\n",
    "            model.add(Dropout(0.1, name=f'block{i}_dropout'))\n",
    "\n",
    "        # ... (rest of the model remains unchanged)\n",
    "\n",
    "\n",
    "        model.add(Flatten(name='flatten'))\n",
    "\n",
    "\n",
    "\n",
    "        # Fully connected layers\n",
    "        for i in range(1, 4):\n",
    "            model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name=f'fc{i}'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i + 8}'))\n",
    "            model.add(Activation('relu', name=f'fc{i}_act'))\n",
    "\n",
    "            # Experiment with different dropout rates\n",
    "            model.add(Dropout(dropout_rate_fc, name=f'fc{i}_dropout'))\n",
    "\n",
    "        # Output layer\n",
    "        #model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc3'))\n",
    "       # model.add(Activation('softmax', name=\"softmax\"))\n",
    "        # Output layer\n",
    "        model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc_final'))\n",
    "        model.add(Activation('softmax', name=\"softmax\"))\n",
    "\n",
    "        print(\"Model built successfully.\")\n",
    "        return model\n",
    "\n",
    "# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\n",
    "input_shape = (5000, 1)\n",
    "num_classes = 10\n",
    "\n",
    "# Experiment with different dropout rates\n",
    "dropout_rate1 = 0.6\n",
    "dropout_rate2 = 0.4\n",
    "dropout_rate_fc = 0.5\n",
    "\n",
    "# Build the model with the specified dropout rates\n",
    "model = DFNet.build(input_shape=input_shape, classes=num_classes,\n",
    "                    dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n",
    "\n",
    "\n",
    "# Print the model summary\n",
    "print(\"Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.002)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\n",
    "input_shape = (5000, 1)\n",
    "num_classes = 95\n",
    "\n",
    "# Experiment with different dropout rates\n",
    "dropout_rate1 = 0.6\n",
    "dropout_rate2 = 0.4\n",
    "dropout_rate_fc = 0.5\n",
    "\n",
    "# Load and merge data using the function you defined\n",
    "X_all, y_all = LoadDataNoDefCW()\n",
    "\n",
    "# Define the number of folds\n",
    "n_splits = 10  # You can adjust this as needed\n",
    "\n",
    "# Initialize Stratified K-Fold\n",
    "stratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results\n",
    "all_histories = []\n",
    "\n",
    "# Iterate over folds\n",
    "for fold, (train_index, test_index) in enumerate(stratkf.split(X_all, y_all)):\n",
    "    print(f\"\\nTraining on Fold {fold + 1}/{n_splits}\")\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test = X_all[train_index], X_all[test_index]\n",
    "    y_train, y_test = y_all[train_index], y_all[test_index]\n",
    "\n",
    "    # Build the model\n",
    "    model = DFNet.build(input_shape=input_shape, classes=num_classes,\n",
    "                        dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.002)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    history = model.fit(X_train, y_train, epochs=20, batch_size=64, \n",
    "                        validation_data=(X_test, y_test), verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(f\"\\nFold {fold + 1} - Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Save history for later analysis if needed\n",
    "    all_histories.append(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7451509b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T18:44:53.593054Z",
     "iopub.status.busy": "2024-01-06T18:44:53.592342Z",
     "iopub.status.idle": "2024-01-06T18:45:04.277126Z",
     "shell.execute_reply": "2024-01-06T18:45:04.275747Z"
    },
    "papermill": {
     "duration": 10.773471,
     "end_time": "2024-01-06T18:45:04.279287",
     "exception": false,
     "start_time": "2024-01-06T18:44:53.505816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading non-defended dataset for closed-world scenario\n",
      "Dataset directory: /kaggle/input/dataset-non-defended-close-world/\n",
      "X_train loaded\n",
      "y_train loaded\n",
      "X_valid loaded\n",
      "y_valid loaded\n",
      "X_test loaded\n",
      "y_test loaded\n",
      "Data dimensions:\n",
      "X: Training data's shape :  (76000, 5000)\n",
      "y: Training data's shape :  (76000,)\n",
      "X: Validation data's shape :  (9500, 5000)\n",
      "y: Validation data's shape :  (9500,)\n",
      "X: Testing data's shape :  (9500, 5000)\n",
      "y: Testing data's shape :  (9500,)\n",
      "Merged data dimensions:\n",
      "X: Merged data's shape :  (95000, 5000)\n",
      "y: Merged data's shape :  (95000,)\n",
      "Features of the merged dataset:\n",
      "[[ 1. -1. -1. ... -1. -1. -1.]\n",
      " [-1.  1.  1. ...  0.  0.  0.]\n",
      " [ 1. -1. -1. ...  0.  0.  0.]\n",
      " ...\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [ 1. -1.  1. ...  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout, ELU\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from keras import layers, models\n",
    "\n",
    "# ... (your existing code)\n",
    "def LoadDataNoDefCW():\n",
    "\n",
    "    print(\"Loading non-defended dataset for closed-world scenario\")\n",
    "\n",
    "    # Point to the directory storing data\n",
    "    dataset_dir = '/kaggle/input/dataset-non-defended-close-world/'\n",
    "\n",
    "    # Debug: Print dataset directory\n",
    "    print(\"Dataset directory:\", dataset_dir)\n",
    "\n",
    "    try:\n",
    "        # Load training data\n",
    "        with open(dataset_dir + 'X_train_NoDef.pkl', 'rb') as handle:\n",
    "            X_train = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"X_train loaded\")\n",
    "\n",
    "        with open(dataset_dir + 'y_train_NoDef.pkl', 'rb') as handle:\n",
    "            y_train = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"y_train loaded\")\n",
    "\n",
    "        # Load validation data\n",
    "        with open(dataset_dir + 'X_valid_NoDef.pkl', 'rb') as handle:\n",
    "            X_valid = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"X_valid loaded\")\n",
    "\n",
    "        with open(dataset_dir + 'y_valid_NoDef.pkl', 'rb') as handle:\n",
    "            y_valid = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"y_valid loaded\")\n",
    "\n",
    "        # Load testing data\n",
    "        with open(dataset_dir + 'X_test_NoDef.pkl', 'rb') as handle:\n",
    "            X_test = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"X_test loaded\")\n",
    "\n",
    "        with open(dataset_dir + 'y_test_NoDef.pkl', 'rb') as handle:\n",
    "            y_test = np.array(pickle.load(handle, encoding='latin1'))\n",
    "        print(\"y_test loaded\")\n",
    "\n",
    "        print(\"Data dimensions:\")\n",
    "        print(\"X: Training data's shape : \", X_train.shape)\n",
    "        print(\"y: Training data's shape : \", y_train.shape)\n",
    "        print(\"X: Validation data's shape : \", X_valid.shape)\n",
    "        print(\"y: Validation data's shape : \", y_valid.shape)\n",
    "        print(\"X: Testing data's shape : \", X_test.shape)\n",
    "        print(\"y: Testing data's shape : \", y_test.shape)\n",
    "\n",
    "        # Merge datasets\n",
    "        X_all = np.concatenate((X_train, X_valid, X_test), axis=0)\n",
    "        y_all = np.concatenate((y_train, y_valid, y_test), axis=0)\n",
    "\n",
    "        print(\"Merged data dimensions:\")\n",
    "        print(\"X: Merged data's shape : \", X_all.shape)\n",
    "        print(\"y: Merged data's shape : \", y_all.shape)\n",
    "        \n",
    "        # Print features of the merged dataset\n",
    "        print(\"Features of the merged dataset:\")\n",
    "        print(X_all)\n",
    "        \n",
    "        # Check if the class distribution is balanced\n",
    "        #unique_classes, class_counts = np.unique(y_all, return_counts=True)\n",
    "       # class_distribution = dict(zip(unique_classes, class_counts))\n",
    "\n",
    "        #print(\"Class distribution:\")\n",
    "        #for class_label, count in class_distribution.items():\n",
    "         #   print(f\"Class {class_label}: {count} samples\")\n",
    "\n",
    "        # Plot the class distribution\n",
    "       # plt.bar(class_distribution.keys(), class_distribution.values())\n",
    "       # plt.xlabel('Class Label')\n",
    "        #plt.ylabel('Number of Samples')\n",
    "        #plt.title('Class Distribution')\n",
    "        #plt.show()\n",
    "        \n",
    "        return X_all, y_all\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        return None\n",
    "\n",
    "# Call the function to load and merge data\n",
    "X_all, y_all = LoadDataNoDefCW()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1094e1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T18:45:04.456632Z",
     "iopub.status.busy": "2024-01-06T18:45:04.456315Z",
     "iopub.status.idle": "2024-01-06T20:39:45.647792Z",
     "shell.execute_reply": "2024-01-06T20:39:45.646767Z"
    },
    "papermill": {
     "duration": 6881.280184,
     "end_time": "2024-01-06T20:39:45.649974",
     "exception": false,
     "start_time": "2024-01-06T18:45:04.369790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built successfully.\n",
      "Model Summary:\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv1D)       (None, 5000, 32)          288       \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 5000, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block1_adv_act1 (ELU)       (None, 5000, 32)          0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv1D)       (None, 5000, 32)          8224      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 5000, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block1_adv_act2 (ELU)       (None, 5000, 32)          0         \n",
      "                                                                 \n",
      " block1_pool (MaxPooling1D)  (None, 1250, 32)          0         \n",
      "                                                                 \n",
      " block1_dropout (Dropout)    (None, 1250, 32)          0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv1D)       (None, 1250, 64)          16448     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 1250, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block2_adv_act1 (ELU)       (None, 1250, 64)          0         \n",
      "                                                                 \n",
      " block2_conv2 (Conv1D)       (None, 1250, 64)          32832     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 1250, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block2_adv_act2 (ELU)       (None, 1250, 64)          0         \n",
      "                                                                 \n",
      " block2_pool (MaxPooling1D)  (None, 313, 64)           0         \n",
      "                                                                 \n",
      " block2_dropout (Dropout)    (None, 313, 64)           0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv1D)       (None, 313, 128)          65664     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 313, 128)          512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block3_adv_act1 (ELU)       (None, 313, 128)          0         \n",
      "                                                                 \n",
      " block3_conv2 (Conv1D)       (None, 313, 128)          131200    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 313, 128)          512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block3_adv_act2 (ELU)       (None, 313, 128)          0         \n",
      "                                                                 \n",
      " block3_pool (MaxPooling1D)  (None, 79, 128)           0         \n",
      "                                                                 \n",
      " block3_dropout (Dropout)    (None, 79, 128)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10112)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 512)               5177856   \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " fc1_act (Activation)        (None, 512)               0         \n",
      "                                                                 \n",
      " fc1_dropout (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " fc2_act (Activation)        (None, 512)               0         \n",
      "                                                                 \n",
      " fc2_dropout (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " fc3 (Dense)                 (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " fc3_act (Activation)        (None, 512)               0         \n",
      "                                                                 \n",
      " fc3_dropout (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " fc_final (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5970890 (22.78 MB)\n",
      "Trainable params: 5966922 (22.76 MB)\n",
      "Non-trainable params: 3968 (15.50 KB)\n",
      "_________________________________________________________________\n",
      "Loading non-defended dataset for closed-world scenario\n",
      "Dataset directory: /kaggle/input/dataset-non-defended-close-world/\n",
      "X_train loaded\n",
      "y_train loaded\n",
      "X_valid loaded\n",
      "y_valid loaded\n",
      "X_test loaded\n",
      "y_test loaded\n",
      "Data dimensions:\n",
      "X: Training data's shape :  (76000, 5000)\n",
      "y: Training data's shape :  (76000,)\n",
      "X: Validation data's shape :  (9500, 5000)\n",
      "y: Validation data's shape :  (9500,)\n",
      "X: Testing data's shape :  (9500, 5000)\n",
      "y: Testing data's shape :  (9500,)\n",
      "Merged data dimensions:\n",
      "X: Merged data's shape :  (95000, 5000)\n",
      "y: Merged data's shape :  (95000,)\n",
      "Features of the merged dataset:\n",
      "[[ 1. -1. -1. ... -1. -1. -1.]\n",
      " [-1.  1.  1. ...  0.  0.  0.]\n",
      " [ 1. -1. -1. ...  0.  0.  0.]\n",
      " ...\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [ 1. -1.  1. ...  0.  0.  0.]]\n",
      "\n",
      "Training on Fold 1/5\n",
      "Model built successfully.\n",
      "Epoch 1/40\n",
      "1188/1188 - 88s - loss: 1.3806 - accuracy: 0.6398 - val_loss: 0.3952 - val_accuracy: 0.9030 - 88s/epoch - 74ms/step\n",
      "Epoch 2/40\n",
      "1188/1188 - 80s - loss: 0.5370 - accuracy: 0.8583 - val_loss: 0.2474 - val_accuracy: 0.9396 - 80s/epoch - 67ms/step\n",
      "Epoch 3/40\n",
      "1188/1188 - 80s - loss: 0.3997 - accuracy: 0.8947 - val_loss: 0.2144 - val_accuracy: 0.9478 - 80s/epoch - 67ms/step\n",
      "Epoch 4/40\n",
      "1188/1188 - 80s - loss: 0.3238 - accuracy: 0.9149 - val_loss: 0.1821 - val_accuracy: 0.9558 - 80s/epoch - 67ms/step\n",
      "Epoch 5/40\n",
      "1188/1188 - 79s - loss: 0.2889 - accuracy: 0.9242 - val_loss: 0.1706 - val_accuracy: 0.9589 - 79s/epoch - 67ms/step\n",
      "Epoch 6/40\n",
      "1188/1188 - 79s - loss: 0.2573 - accuracy: 0.9328 - val_loss: 0.1477 - val_accuracy: 0.9660 - 79s/epoch - 67ms/step\n",
      "Epoch 7/40\n",
      "1188/1188 - 79s - loss: 0.2285 - accuracy: 0.9398 - val_loss: 0.1459 - val_accuracy: 0.9654 - 79s/epoch - 67ms/step\n",
      "Epoch 8/40\n",
      "1188/1188 - 79s - loss: 0.2074 - accuracy: 0.9447 - val_loss: 0.1395 - val_accuracy: 0.9659 - 79s/epoch - 67ms/step\n",
      "Epoch 9/40\n",
      "1188/1188 - 79s - loss: 0.1915 - accuracy: 0.9490 - val_loss: 0.1252 - val_accuracy: 0.9694 - 79s/epoch - 67ms/step\n",
      "Epoch 10/40\n",
      "1188/1188 - 79s - loss: 0.1776 - accuracy: 0.9529 - val_loss: 0.1252 - val_accuracy: 0.9688 - 79s/epoch - 67ms/step\n",
      "Epoch 11/40\n",
      "1188/1188 - 79s - loss: 0.1671 - accuracy: 0.9552 - val_loss: 0.1376 - val_accuracy: 0.9676 - 79s/epoch - 67ms/step\n",
      "Epoch 12/40\n",
      "1188/1188 - 79s - loss: 0.1586 - accuracy: 0.9568 - val_loss: 0.1298 - val_accuracy: 0.9691 - 79s/epoch - 67ms/step\n",
      "594/594 - 5s - loss: 0.1252 - accuracy: 0.9694 - 5s/epoch - 9ms/step\n",
      "\n",
      "Fold 1 - Test Accuracy: 96.94%\n",
      "\n",
      "Training on Fold 2/5\n",
      "Model built successfully.\n",
      "Epoch 1/40\n",
      "1188/1188 - 88s - loss: 1.3021 - accuracy: 0.6580 - val_loss: 0.3888 - val_accuracy: 0.8975 - 88s/epoch - 74ms/step\n",
      "Epoch 2/40\n",
      "1188/1188 - 79s - loss: 0.5216 - accuracy: 0.8641 - val_loss: 0.2439 - val_accuracy: 0.9379 - 79s/epoch - 67ms/step\n",
      "Epoch 3/40\n",
      "1188/1188 - 79s - loss: 0.3918 - accuracy: 0.8977 - val_loss: 0.1873 - val_accuracy: 0.9532 - 79s/epoch - 67ms/step\n",
      "Epoch 4/40\n",
      "1188/1188 - 80s - loss: 0.3242 - accuracy: 0.9157 - val_loss: 0.1955 - val_accuracy: 0.9485 - 80s/epoch - 67ms/step\n",
      "Epoch 5/40\n",
      "1188/1188 - 79s - loss: 0.2853 - accuracy: 0.9244 - val_loss: 0.1917 - val_accuracy: 0.9493 - 79s/epoch - 67ms/step\n",
      "Epoch 6/40\n",
      "1188/1188 - 79s - loss: 0.2488 - accuracy: 0.9341 - val_loss: 0.1418 - val_accuracy: 0.9660 - 79s/epoch - 67ms/step\n",
      "Epoch 7/40\n",
      "1188/1188 - 79s - loss: 0.2283 - accuracy: 0.9396 - val_loss: 0.1348 - val_accuracy: 0.9671 - 79s/epoch - 67ms/step\n",
      "Epoch 8/40\n",
      "1188/1188 - 79s - loss: 0.2066 - accuracy: 0.9457 - val_loss: 0.1365 - val_accuracy: 0.9663 - 79s/epoch - 67ms/step\n",
      "Epoch 9/40\n",
      "1188/1188 - 79s - loss: 0.1897 - accuracy: 0.9497 - val_loss: 0.1163 - val_accuracy: 0.9708 - 79s/epoch - 67ms/step\n",
      "Epoch 10/40\n",
      "1188/1188 - 79s - loss: 0.1785 - accuracy: 0.9531 - val_loss: 0.1107 - val_accuracy: 0.9732 - 79s/epoch - 67ms/step\n",
      "Epoch 11/40\n",
      "1188/1188 - 79s - loss: 0.1698 - accuracy: 0.9547 - val_loss: 0.1176 - val_accuracy: 0.9702 - 79s/epoch - 67ms/step\n",
      "Epoch 12/40\n",
      "1188/1188 - 79s - loss: 0.1605 - accuracy: 0.9577 - val_loss: 0.1068 - val_accuracy: 0.9753 - 79s/epoch - 67ms/step\n",
      "Epoch 13/40\n",
      "1188/1188 - 79s - loss: 0.1504 - accuracy: 0.9596 - val_loss: 0.1253 - val_accuracy: 0.9688 - 79s/epoch - 67ms/step\n",
      "Epoch 14/40\n",
      "1188/1188 - 79s - loss: 0.1438 - accuracy: 0.9617 - val_loss: 0.1083 - val_accuracy: 0.9726 - 79s/epoch - 67ms/step\n",
      "Epoch 15/40\n",
      "1188/1188 - 79s - loss: 0.1337 - accuracy: 0.9647 - val_loss: 0.1067 - val_accuracy: 0.9746 - 79s/epoch - 67ms/step\n",
      "Epoch 16/40\n",
      "1188/1188 - 80s - loss: 0.1293 - accuracy: 0.9658 - val_loss: 0.0983 - val_accuracy: 0.9760 - 80s/epoch - 67ms/step\n",
      "Epoch 17/40\n",
      "1188/1188 - 79s - loss: 0.1247 - accuracy: 0.9667 - val_loss: 0.0921 - val_accuracy: 0.9780 - 79s/epoch - 67ms/step\n",
      "Epoch 18/40\n",
      "1188/1188 - 79s - loss: 0.1171 - accuracy: 0.9685 - val_loss: 0.0952 - val_accuracy: 0.9785 - 79s/epoch - 67ms/step\n",
      "Epoch 19/40\n",
      "1188/1188 - 79s - loss: 0.1196 - accuracy: 0.9685 - val_loss: 0.0940 - val_accuracy: 0.9770 - 79s/epoch - 67ms/step\n",
      "Epoch 20/40\n",
      "1188/1188 - 80s - loss: 0.1129 - accuracy: 0.9699 - val_loss: 0.0945 - val_accuracy: 0.9786 - 80s/epoch - 67ms/step\n",
      "594/594 - 5s - loss: 0.0921 - accuracy: 0.9780 - 5s/epoch - 9ms/step\n",
      "\n",
      "Fold 2 - Test Accuracy: 97.80%\n",
      "\n",
      "Training on Fold 3/5\n",
      "Model built successfully.\n",
      "Epoch 1/40\n",
      "1188/1188 - 90s - loss: 1.3110 - accuracy: 0.6549 - val_loss: 0.3502 - val_accuracy: 0.9156 - 90s/epoch - 76ms/step\n",
      "Epoch 2/40\n",
      "1188/1188 - 80s - loss: 0.5067 - accuracy: 0.8674 - val_loss: 0.2446 - val_accuracy: 0.9384 - 80s/epoch - 67ms/step\n",
      "Epoch 3/40\n",
      "1188/1188 - 80s - loss: 0.3827 - accuracy: 0.8998 - val_loss: 0.2109 - val_accuracy: 0.9458 - 80s/epoch - 67ms/step\n",
      "Epoch 4/40\n",
      "1188/1188 - 80s - loss: 0.3124 - accuracy: 0.9179 - val_loss: 0.1721 - val_accuracy: 0.9569 - 80s/epoch - 67ms/step\n",
      "Epoch 5/40\n",
      "1188/1188 - 80s - loss: 0.2724 - accuracy: 0.9273 - val_loss: 0.1479 - val_accuracy: 0.9621 - 80s/epoch - 67ms/step\n",
      "Epoch 6/40\n",
      "1188/1188 - 80s - loss: 0.2432 - accuracy: 0.9366 - val_loss: 0.1576 - val_accuracy: 0.9615 - 80s/epoch - 67ms/step\n",
      "Epoch 7/40\n",
      "1188/1188 - 80s - loss: 0.2227 - accuracy: 0.9414 - val_loss: 0.1346 - val_accuracy: 0.9655 - 80s/epoch - 67ms/step\n",
      "Epoch 8/40\n",
      "1188/1188 - 80s - loss: 0.1979 - accuracy: 0.9469 - val_loss: 0.1393 - val_accuracy: 0.9668 - 80s/epoch - 67ms/step\n",
      "Epoch 9/40\n",
      "1188/1188 - 80s - loss: 0.1863 - accuracy: 0.9509 - val_loss: 0.1223 - val_accuracy: 0.9686 - 80s/epoch - 67ms/step\n",
      "Epoch 10/40\n",
      "1188/1188 - 80s - loss: 0.1725 - accuracy: 0.9538 - val_loss: 0.1230 - val_accuracy: 0.9703 - 80s/epoch - 67ms/step\n",
      "Epoch 11/40\n",
      "1188/1188 - 79s - loss: 0.1643 - accuracy: 0.9565 - val_loss: 0.1111 - val_accuracy: 0.9718 - 79s/epoch - 67ms/step\n",
      "Epoch 12/40\n",
      "1188/1188 - 79s - loss: 0.1538 - accuracy: 0.9588 - val_loss: 0.1156 - val_accuracy: 0.9729 - 79s/epoch - 67ms/step\n",
      "Epoch 13/40\n",
      "1188/1188 - 80s - loss: 0.1459 - accuracy: 0.9612 - val_loss: 0.1054 - val_accuracy: 0.9734 - 80s/epoch - 67ms/step\n",
      "Epoch 14/40\n",
      "1188/1188 - 79s - loss: 0.1340 - accuracy: 0.9641 - val_loss: 0.1091 - val_accuracy: 0.9751 - 79s/epoch - 67ms/step\n",
      "Epoch 15/40\n",
      "1188/1188 - 79s - loss: 0.1351 - accuracy: 0.9638 - val_loss: 0.1094 - val_accuracy: 0.9735 - 79s/epoch - 67ms/step\n",
      "Epoch 16/40\n",
      "1188/1188 - 79s - loss: 0.1222 - accuracy: 0.9670 - val_loss: 0.1036 - val_accuracy: 0.9750 - 79s/epoch - 67ms/step\n",
      "Epoch 17/40\n",
      "1188/1188 - 80s - loss: 0.1208 - accuracy: 0.9684 - val_loss: 0.1055 - val_accuracy: 0.9753 - 80s/epoch - 67ms/step\n",
      "Epoch 18/40\n",
      "1188/1188 - 80s - loss: 0.1158 - accuracy: 0.9697 - val_loss: 0.0998 - val_accuracy: 0.9754 - 80s/epoch - 67ms/step\n",
      "Epoch 19/40\n",
      "1188/1188 - 79s - loss: 0.1087 - accuracy: 0.9708 - val_loss: 0.1071 - val_accuracy: 0.9752 - 79s/epoch - 67ms/step\n",
      "Epoch 20/40\n",
      "1188/1188 - 79s - loss: 0.1097 - accuracy: 0.9706 - val_loss: 0.1050 - val_accuracy: 0.9761 - 79s/epoch - 67ms/step\n",
      "Epoch 21/40\n",
      "1188/1188 - 79s - loss: 0.1036 - accuracy: 0.9724 - val_loss: 0.1021 - val_accuracy: 0.9769 - 79s/epoch - 67ms/step\n",
      "594/594 - 5s - loss: 0.0998 - accuracy: 0.9754 - 5s/epoch - 9ms/step\n",
      "\n",
      "Fold 3 - Test Accuracy: 97.54%\n",
      "\n",
      "Training on Fold 4/5\n",
      "Model built successfully.\n",
      "Epoch 1/40\n",
      "1188/1188 - 89s - loss: 1.3201 - accuracy: 0.6532 - val_loss: 0.3667 - val_accuracy: 0.9073 - 89s/epoch - 75ms/step\n",
      "Epoch 2/40\n",
      "1188/1188 - 80s - loss: 0.5359 - accuracy: 0.8589 - val_loss: 0.2418 - val_accuracy: 0.9379 - 80s/epoch - 67ms/step\n",
      "Epoch 3/40\n",
      "1188/1188 - 80s - loss: 0.4044 - accuracy: 0.8936 - val_loss: 0.1923 - val_accuracy: 0.9510 - 80s/epoch - 67ms/step\n",
      "Epoch 4/40\n",
      "1188/1188 - 79s - loss: 0.3351 - accuracy: 0.9131 - val_loss: 0.2013 - val_accuracy: 0.9489 - 79s/epoch - 67ms/step\n",
      "Epoch 5/40\n",
      "1188/1188 - 79s - loss: 0.2898 - accuracy: 0.9237 - val_loss: 0.1484 - val_accuracy: 0.9622 - 79s/epoch - 67ms/step\n",
      "Epoch 6/40\n",
      "1188/1188 - 79s - loss: 0.2547 - accuracy: 0.9334 - val_loss: 0.1401 - val_accuracy: 0.9655 - 79s/epoch - 67ms/step\n",
      "Epoch 7/40\n",
      "1188/1188 - 79s - loss: 0.2280 - accuracy: 0.9401 - val_loss: 0.1290 - val_accuracy: 0.9673 - 79s/epoch - 67ms/step\n",
      "Epoch 8/40\n",
      "1188/1188 - 79s - loss: 0.2105 - accuracy: 0.9445 - val_loss: 0.1245 - val_accuracy: 0.9693 - 79s/epoch - 67ms/step\n",
      "Epoch 9/40\n",
      "1188/1188 - 79s - loss: 0.1958 - accuracy: 0.9485 - val_loss: 0.1193 - val_accuracy: 0.9713 - 79s/epoch - 67ms/step\n",
      "Epoch 10/40\n",
      "1188/1188 - 80s - loss: 0.1775 - accuracy: 0.9527 - val_loss: 0.1215 - val_accuracy: 0.9693 - 80s/epoch - 67ms/step\n",
      "Epoch 11/40\n",
      "1188/1188 - 80s - loss: 0.1721 - accuracy: 0.9549 - val_loss: 0.1098 - val_accuracy: 0.9720 - 80s/epoch - 67ms/step\n",
      "Epoch 12/40\n",
      "1188/1188 - 80s - loss: 0.1610 - accuracy: 0.9575 - val_loss: 0.1152 - val_accuracy: 0.9706 - 80s/epoch - 67ms/step\n",
      "Epoch 13/40\n",
      "1188/1188 - 80s - loss: 0.1517 - accuracy: 0.9594 - val_loss: 0.1113 - val_accuracy: 0.9726 - 80s/epoch - 67ms/step\n",
      "Epoch 14/40\n",
      "1188/1188 - 80s - loss: 0.1454 - accuracy: 0.9614 - val_loss: 0.1090 - val_accuracy: 0.9733 - 80s/epoch - 67ms/step\n",
      "Epoch 15/40\n",
      "1188/1188 - 80s - loss: 0.1407 - accuracy: 0.9634 - val_loss: 0.1039 - val_accuracy: 0.9743 - 80s/epoch - 67ms/step\n",
      "Epoch 16/40\n",
      "1188/1188 - 80s - loss: 0.1327 - accuracy: 0.9639 - val_loss: 0.1010 - val_accuracy: 0.9756 - 80s/epoch - 67ms/step\n",
      "Epoch 17/40\n",
      "1188/1188 - 80s - loss: 0.1266 - accuracy: 0.9660 - val_loss: 0.1061 - val_accuracy: 0.9750 - 80s/epoch - 67ms/step\n",
      "Epoch 18/40\n",
      "1188/1188 - 80s - loss: 0.1228 - accuracy: 0.9668 - val_loss: 0.1034 - val_accuracy: 0.9747 - 80s/epoch - 67ms/step\n",
      "Epoch 19/40\n",
      "1188/1188 - 80s - loss: 0.1195 - accuracy: 0.9676 - val_loss: 0.1038 - val_accuracy: 0.9756 - 80s/epoch - 67ms/step\n",
      "594/594 - 5s - loss: 0.1010 - accuracy: 0.9756 - 5s/epoch - 9ms/step\n",
      "\n",
      "Fold 4 - Test Accuracy: 97.56%\n",
      "\n",
      "Training on Fold 5/5\n",
      "Model built successfully.\n",
      "Epoch 1/40\n",
      "1188/1188 - 88s - loss: 1.3418 - accuracy: 0.6469 - val_loss: 0.3566 - val_accuracy: 0.9086 - 88s/epoch - 74ms/step\n",
      "Epoch 2/40\n",
      "1188/1188 - 79s - loss: 0.5252 - accuracy: 0.8616 - val_loss: 0.2317 - val_accuracy: 0.9416 - 79s/epoch - 67ms/step\n",
      "Epoch 3/40\n",
      "1188/1188 - 79s - loss: 0.3870 - accuracy: 0.8986 - val_loss: 0.2015 - val_accuracy: 0.9513 - 79s/epoch - 67ms/step\n",
      "Epoch 4/40\n",
      "1188/1188 - 79s - loss: 0.3237 - accuracy: 0.9146 - val_loss: 0.1533 - val_accuracy: 0.9621 - 79s/epoch - 67ms/step\n",
      "Epoch 5/40\n",
      "1188/1188 - 79s - loss: 0.2802 - accuracy: 0.9270 - val_loss: 0.1442 - val_accuracy: 0.9638 - 79s/epoch - 67ms/step\n",
      "Epoch 6/40\n",
      "1188/1188 - 80s - loss: 0.2520 - accuracy: 0.9336 - val_loss: 0.1310 - val_accuracy: 0.9678 - 80s/epoch - 67ms/step\n",
      "Epoch 7/40\n",
      "1188/1188 - 80s - loss: 0.2264 - accuracy: 0.9398 - val_loss: 0.1247 - val_accuracy: 0.9695 - 80s/epoch - 67ms/step\n",
      "Epoch 8/40\n",
      "1188/1188 - 79s - loss: 0.2056 - accuracy: 0.9454 - val_loss: 0.1143 - val_accuracy: 0.9734 - 79s/epoch - 67ms/step\n",
      "Epoch 9/40\n",
      "1188/1188 - 79s - loss: 0.1877 - accuracy: 0.9497 - val_loss: 0.1333 - val_accuracy: 0.9682 - 79s/epoch - 67ms/step\n",
      "Epoch 10/40\n",
      "1188/1188 - 79s - loss: 0.1769 - accuracy: 0.9538 - val_loss: 0.1086 - val_accuracy: 0.9745 - 79s/epoch - 67ms/step\n",
      "Epoch 11/40\n",
      "1188/1188 - 79s - loss: 0.1680 - accuracy: 0.9556 - val_loss: 0.1102 - val_accuracy: 0.9726 - 79s/epoch - 67ms/step\n",
      "Epoch 12/40\n",
      "1188/1188 - 79s - loss: 0.1543 - accuracy: 0.9587 - val_loss: 0.1105 - val_accuracy: 0.9751 - 79s/epoch - 67ms/step\n",
      "Epoch 13/40\n",
      "1188/1188 - 79s - loss: 0.1474 - accuracy: 0.9599 - val_loss: 0.1162 - val_accuracy: 0.9727 - 79s/epoch - 67ms/step\n",
      "594/594 - 5s - loss: 0.1086 - accuracy: 0.9745 - 5s/epoch - 9ms/step\n",
      "\n",
      "Fold 5 - Test Accuracy: 97.45%\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# ... (previous imports and code remain unchanged)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout, ELU\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DFNet:\n",
    "    @staticmethod\n",
    "    def build(input_shape, classes, dropout_rate1=0.7, dropout_rate2=0.5, dropout_rate_fc=0.5):\n",
    "        model = Sequential()\n",
    "\n",
    "        # ... (previous model definition remains unchanged)\n",
    "        # Block 1\n",
    "        filter_num = [None, 32, 64, 128, 256]\n",
    "        kernel_size = [None, 8, 8, 8, 8]\n",
    "        conv_stride_size = [None, 1, 1, 1, 1]\n",
    "        pool_stride_size = [None, 4, 4, 4, 4]\n",
    "        pool_size = [None, 8, 8, 8, 8]\n",
    "\n",
    "        for i in range(1, 4):\n",
    "            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n",
    "                             strides=conv_stride_size[i], padding='same',\n",
    "                             input_shape=input_shape if i == 1 else (None, input_shape[1]),\n",
    "                             name=f'block{i}_conv1'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i * 2 - 1}'))\n",
    "            model.add(ELU(alpha=1.0, name=f'block{i}_adv_act1'))\n",
    "\n",
    "            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n",
    "                             strides=conv_stride_size[i], padding='same',\n",
    "                             name=f'block{i}_conv2'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i * 2}'))\n",
    "            model.add(ELU(alpha=1.0, name=f'block{i}_adv_act2'))\n",
    "\n",
    "            model.add(MaxPooling1D(pool_size=pool_size[i], strides=pool_stride_size[i],\n",
    "                                   padding='same', name=f'block{i}_pool'))\n",
    "            model.add(Dropout(0.1, name=f'block{i}_dropout'))\n",
    "\n",
    "        # ... (rest of the model remains unchanged)\n",
    "\n",
    "\n",
    "        model.add(Flatten(name='flatten'))\n",
    "\n",
    "\n",
    "\n",
    "        # Fully connected layers\n",
    "        for i in range(1, 4):\n",
    "            model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name=f'fc{i}'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i + 8}'))\n",
    "            model.add(Activation('relu', name=f'fc{i}_act'))\n",
    "\n",
    "            # Experiment with different dropout rates\n",
    "            model.add(Dropout(dropout_rate_fc, name=f'fc{i}_dropout'))\n",
    "\n",
    "        # Output layer\n",
    "        #model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc3'))\n",
    "       # model.add(Activation('softmax', name=\"softmax\"))\n",
    "        # Output layer\n",
    "        model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc_final'))\n",
    "        model.add(Activation('softmax', name=\"softmax\"))\n",
    "\n",
    "        print(\"Model built successfully.\")\n",
    "        return model\n",
    "\n",
    "# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\n",
    "input_shape = (5000, 1)\n",
    "num_classes = 10\n",
    "\n",
    "# Experiment with different dropout rates\n",
    "dropout_rate1 = 0.6\n",
    "dropout_rate2 = 0.4\n",
    "dropout_rate_fc = 0.5\n",
    "\n",
    "# Build the model with the specified dropout rates\n",
    "model = DFNet.build(input_shape=input_shape, classes=num_classes,\n",
    "                    dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n",
    "\n",
    "\n",
    "# Print the model summary\n",
    "print(\"Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.002)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\n",
    "input_shape = (5000, 1)\n",
    "num_classes = 95\n",
    "\n",
    "# Experiment with different dropout rates\n",
    "dropout_rate1 = 0.6\n",
    "dropout_rate2 = 0.4\n",
    "dropout_rate_fc = 0.5\n",
    "\n",
    "# Load and merge data using the function you defined\n",
    "X_all, y_all = LoadDataNoDefCW()\n",
    "\n",
    "# Define the number of folds\n",
    "n_splits = 5  # You can adjust this as needed\n",
    "\n",
    "# Initialize Stratified K-Fold\n",
    "stratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results\n",
    "all_histories = []\n",
    "\n",
    "# Iterate over folds\n",
    "for fold, (train_index, test_index) in enumerate(stratkf.split(X_all, y_all)):\n",
    "    print(f\"\\nTraining on Fold {fold + 1}/{n_splits}\")\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test = X_all[train_index], X_all[test_index]\n",
    "    y_train, y_test = y_all[train_index], y_all[test_index]\n",
    "\n",
    "    # Build the model\n",
    "    model = DFNet.build(input_shape=input_shape, classes=num_classes,\n",
    "                        dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.002)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    history = model.fit(X_train, y_train, epochs=40, batch_size=64, \n",
    "                        validation_data=(X_test, y_test), verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(f\"\\nFold {fold + 1} - Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Save history for later analysis if needed\n",
    "    all_histories.append(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83631658",
   "metadata": {
    "papermill": {
     "duration": 0.104395,
     "end_time": "2024-01-06T20:39:45.861320",
     "exception": false,
     "start_time": "2024-01-06T20:39:45.756925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ecc40",
   "metadata": {
    "papermill": {
     "duration": 0.105342,
     "end_time": "2024-01-06T20:39:46.071330",
     "exception": false,
     "start_time": "2024-01-06T20:39:45.965988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e438f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T20:39:46.288165Z",
     "iopub.status.busy": "2024-01-06T20:39:46.287798Z"
    },
    "papermill": {
     "duration": 3408.377251,
     "end_time": "2024-01-06T21:36:34.555917",
     "exception": false,
     "start_time": "2024-01-06T20:39:46.178666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built successfully.\n",
      "Model Summary:\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv1D)       (None, 5000, 32)          288       \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 5000, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block1_adv_act1 (ELU)       (None, 5000, 32)          0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv1D)       (None, 5000, 32)          8224      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 5000, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block1_adv_act2 (ELU)       (None, 5000, 32)          0         \n",
      "                                                                 \n",
      " block1_pool (MaxPooling1D)  (None, 1250, 32)          0         \n",
      "                                                                 \n",
      " block1_dropout (Dropout)    (None, 1250, 32)          0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv1D)       (None, 1250, 64)          16448     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 1250, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block2_adv_act1 (ELU)       (None, 1250, 64)          0         \n",
      "                                                                 \n",
      " block2_conv2 (Conv1D)       (None, 1250, 64)          32832     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 1250, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block2_adv_act2 (ELU)       (None, 1250, 64)          0         \n",
      "                                                                 \n",
      " block2_pool (MaxPooling1D)  (None, 313, 64)           0         \n",
      "                                                                 \n",
      " block2_dropout (Dropout)    (None, 313, 64)           0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv1D)       (None, 313, 128)          65664     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 313, 128)          512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block3_adv_act1 (ELU)       (None, 313, 128)          0         \n",
      "                                                                 \n",
      " block3_conv2 (Conv1D)       (None, 313, 128)          131200    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 313, 128)          512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " block3_adv_act2 (ELU)       (None, 313, 128)          0         \n",
      "                                                                 \n",
      " block3_pool (MaxPooling1D)  (None, 79, 128)           0         \n",
      "                                                                 \n",
      " block3_dropout (Dropout)    (None, 79, 128)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10112)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 512)               5177856   \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " fc1_act (Activation)        (None, 512)               0         \n",
      "                                                                 \n",
      " fc1_dropout (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " fc2_act (Activation)        (None, 512)               0         \n",
      "                                                                 \n",
      " fc2_dropout (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " fc3 (Dense)                 (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " fc3_act (Activation)        (None, 512)               0         \n",
      "                                                                 \n",
      " fc3_dropout (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " fc_final (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5970890 (22.78 MB)\n",
      "Trainable params: 5966922 (22.76 MB)\n",
      "Non-trainable params: 3968 (15.50 KB)\n",
      "_________________________________________________________________\n",
      "Loading non-defended dataset for closed-world scenario\n",
      "Dataset directory: /kaggle/input/dataset-non-defended-close-world/\n",
      "X_train loaded\n",
      "y_train loaded\n",
      "X_valid loaded\n",
      "y_valid loaded\n",
      "X_test loaded\n",
      "y_test loaded\n",
      "Data dimensions:\n",
      "X: Training data's shape :  (76000, 5000)\n",
      "y: Training data's shape :  (76000,)\n",
      "X: Validation data's shape :  (9500, 5000)\n",
      "y: Validation data's shape :  (9500,)\n",
      "X: Testing data's shape :  (9500, 5000)\n",
      "y: Testing data's shape :  (9500,)\n",
      "Merged data dimensions:\n",
      "X: Merged data's shape :  (95000, 5000)\n",
      "y: Merged data's shape :  (95000,)\n",
      "Features of the merged dataset:\n",
      "[[ 1. -1. -1. ... -1. -1. -1.]\n",
      " [-1.  1.  1. ...  0.  0.  0.]\n",
      " [ 1. -1. -1. ...  0.  0.  0.]\n",
      " ...\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [ 1. -1.  1. ...  0.  0.  0.]]\n",
      "\n",
      "Training on Fold 1/5\n",
      "Model built successfully.\n",
      "Epoch 1/40\n",
      "1188/1188 - 88s - loss: 1.3577 - accuracy: 0.6427 - val_loss: 0.5294 - val_accuracy: 0.8614 - 88s/epoch - 74ms/step\n",
      "Epoch 2/40\n",
      "1188/1188 - 79s - loss: 0.5407 - accuracy: 0.8579 - val_loss: 0.2614 - val_accuracy: 0.9373 - 79s/epoch - 67ms/step\n",
      "Epoch 3/40\n",
      "1188/1188 - 79s - loss: 0.4042 - accuracy: 0.8925 - val_loss: 0.2316 - val_accuracy: 0.9403 - 79s/epoch - 67ms/step\n",
      "Epoch 4/40\n",
      "1188/1188 - 79s - loss: 0.3318 - accuracy: 0.9127 - val_loss: 0.1752 - val_accuracy: 0.9542 - 79s/epoch - 67ms/step\n",
      "Epoch 5/40\n",
      "1188/1188 - 79s - loss: 0.2858 - accuracy: 0.9256 - val_loss: 0.1609 - val_accuracy: 0.9594 - 79s/epoch - 67ms/step\n",
      "Epoch 6/40\n",
      "1188/1188 - 79s - loss: 0.2509 - accuracy: 0.9331 - val_loss: 0.1566 - val_accuracy: 0.9642 - 79s/epoch - 67ms/step\n",
      "Epoch 7/40\n",
      "1188/1188 - 79s - loss: 0.2319 - accuracy: 0.9387 - val_loss: 0.1460 - val_accuracy: 0.9666 - 79s/epoch - 67ms/step\n",
      "Epoch 8/40\n",
      "1188/1188 - 79s - loss: 0.2061 - accuracy: 0.9452 - val_loss: 0.1262 - val_accuracy: 0.9689 - 79s/epoch - 67ms/step\n",
      "Epoch 9/40\n",
      "1188/1188 - 79s - loss: 0.1953 - accuracy: 0.9481 - val_loss: 0.1213 - val_accuracy: 0.9715 - 79s/epoch - 67ms/step\n",
      "Epoch 10/40\n",
      "1188/1188 - 79s - loss: 0.1825 - accuracy: 0.9513 - val_loss: 0.1242 - val_accuracy: 0.9729 - 79s/epoch - 67ms/step\n",
      "Epoch 11/40\n",
      "1188/1188 - 79s - loss: 0.1692 - accuracy: 0.9547 - val_loss: 0.1170 - val_accuracy: 0.9729 - 79s/epoch - 67ms/step\n",
      "Epoch 12/40\n",
      "1188/1188 - 79s - loss: 0.1581 - accuracy: 0.9576 - val_loss: 0.1182 - val_accuracy: 0.9718 - 79s/epoch - 67ms/step\n",
      "Epoch 13/40\n",
      "1188/1188 - 79s - loss: 0.1530 - accuracy: 0.9589 - val_loss: 0.1127 - val_accuracy: 0.9751 - 79s/epoch - 67ms/step\n",
      "Epoch 14/40\n",
      "1188/1188 - 79s - loss: 0.1418 - accuracy: 0.9622 - val_loss: 0.1201 - val_accuracy: 0.9726 - 79s/epoch - 67ms/step\n",
      "Epoch 15/40\n",
      "1188/1188 - 79s - loss: 0.1358 - accuracy: 0.9641 - val_loss: 0.1106 - val_accuracy: 0.9748 - 79s/epoch - 67ms/step\n",
      "Epoch 16/40\n",
      "1188/1188 - 79s - loss: 0.1300 - accuracy: 0.9647 - val_loss: 0.1124 - val_accuracy: 0.9758 - 79s/epoch - 67ms/step\n",
      "Epoch 17/40\n",
      "1188/1188 - 79s - loss: 0.1238 - accuracy: 0.9679 - val_loss: 0.1082 - val_accuracy: 0.9758 - 79s/epoch - 67ms/step\n",
      "Epoch 18/40\n",
      "1188/1188 - 79s - loss: 0.1188 - accuracy: 0.9679 - val_loss: 0.1105 - val_accuracy: 0.9758 - 79s/epoch - 67ms/step\n",
      "Epoch 19/40\n",
      "1188/1188 - 79s - loss: 0.1200 - accuracy: 0.9685 - val_loss: 0.1026 - val_accuracy: 0.9773 - 79s/epoch - 67ms/step\n",
      "Epoch 20/40\n",
      "1188/1188 - 79s - loss: 0.1108 - accuracy: 0.9702 - val_loss: 0.1005 - val_accuracy: 0.9779 - 79s/epoch - 67ms/step\n",
      "Epoch 21/40\n",
      "1188/1188 - 79s - loss: 0.1060 - accuracy: 0.9716 - val_loss: 0.1032 - val_accuracy: 0.9770 - 79s/epoch - 67ms/step\n",
      "Epoch 22/40\n",
      "1188/1188 - 79s - loss: 0.1046 - accuracy: 0.9727 - val_loss: 0.1029 - val_accuracy: 0.9772 - 79s/epoch - 67ms/step\n",
      "Epoch 23/40\n",
      "1188/1188 - 79s - loss: 0.1017 - accuracy: 0.9732 - val_loss: 0.1040 - val_accuracy: 0.9771 - 79s/epoch - 67ms/step\n",
      "594/594 - 5s - loss: 0.1005 - accuracy: 0.9779 - 5s/epoch - 9ms/step\n",
      "\n",
      "Fold 1 - Test Accuracy: 97.79%\n",
      "\n",
      "Training on Fold 2/5\n",
      "Model built successfully.\n",
      "Epoch 1/40\n",
      "1188/1188 - 88s - loss: 1.3585 - accuracy: 0.6439 - val_loss: 0.3806 - val_accuracy: 0.9067 - 88s/epoch - 74ms/step\n",
      "Epoch 2/40\n",
      "1188/1188 - 80s - loss: 0.5563 - accuracy: 0.8549 - val_loss: 0.2585 - val_accuracy: 0.9316 - 80s/epoch - 67ms/step\n",
      "Epoch 3/40\n",
      "1188/1188 - 79s - loss: 0.4095 - accuracy: 0.8928 - val_loss: 0.1982 - val_accuracy: 0.9476 - 79s/epoch - 67ms/step\n",
      "Epoch 4/40\n",
      "1188/1188 - 80s - loss: 0.3390 - accuracy: 0.9121 - val_loss: 0.1630 - val_accuracy: 0.9580 - 80s/epoch - 67ms/step\n",
      "Epoch 5/40\n",
      "1188/1188 - 80s - loss: 0.2911 - accuracy: 0.9243 - val_loss: 0.1475 - val_accuracy: 0.9629 - 80s/epoch - 67ms/step\n",
      "Epoch 6/40\n",
      "1188/1188 - 79s - loss: 0.2565 - accuracy: 0.9336 - val_loss: 0.1623 - val_accuracy: 0.9581 - 79s/epoch - 67ms/step\n",
      "Epoch 7/40\n",
      "1188/1188 - 79s - loss: 0.2319 - accuracy: 0.9393 - val_loss: 0.1312 - val_accuracy: 0.9683 - 79s/epoch - 67ms/step\n",
      "Epoch 8/40\n",
      "1188/1188 - 79s - loss: 0.2117 - accuracy: 0.9438 - val_loss: 0.1250 - val_accuracy: 0.9697 - 79s/epoch - 67ms/step\n",
      "Epoch 9/40\n",
      "1188/1188 - 79s - loss: 0.1933 - accuracy: 0.9490 - val_loss: 0.1165 - val_accuracy: 0.9720 - 79s/epoch - 67ms/step\n",
      "Epoch 10/40\n",
      "1188/1188 - 79s - loss: 0.1810 - accuracy: 0.9520 - val_loss: 0.1140 - val_accuracy: 0.9721 - 79s/epoch - 67ms/step\n",
      "Epoch 11/40\n",
      "1188/1188 - 79s - loss: 0.1671 - accuracy: 0.9559 - val_loss: 0.1141 - val_accuracy: 0.9729 - 79s/epoch - 67ms/step\n",
      "Epoch 12/40\n",
      "1188/1188 - 79s - loss: 0.1624 - accuracy: 0.9569 - val_loss: 0.1128 - val_accuracy: 0.9723 - 79s/epoch - 67ms/step\n",
      "Epoch 13/40\n",
      "1188/1188 - 79s - loss: 0.1518 - accuracy: 0.9597 - val_loss: 0.1082 - val_accuracy: 0.9745 - 79s/epoch - 67ms/step\n",
      "Epoch 14/40\n",
      "1188/1188 - 79s - loss: 0.1396 - accuracy: 0.9621 - val_loss: 0.1061 - val_accuracy: 0.9734 - 79s/epoch - 67ms/step\n",
      "Epoch 15/40\n",
      "1188/1188 - 79s - loss: 0.1373 - accuracy: 0.9638 - val_loss: 0.1016 - val_accuracy: 0.9755 - 79s/epoch - 67ms/step\n",
      "Epoch 16/40\n",
      "1188/1188 - 79s - loss: 0.1328 - accuracy: 0.9652 - val_loss: 0.0978 - val_accuracy: 0.9766 - 79s/epoch - 67ms/step\n",
      "Epoch 17/40\n",
      "1188/1188 - 79s - loss: 0.1253 - accuracy: 0.9666 - val_loss: 0.1002 - val_accuracy: 0.9759 - 79s/epoch - 67ms/step\n",
      "Epoch 18/40\n",
      "1188/1188 - 79s - loss: 0.1227 - accuracy: 0.9672 - val_loss: 0.1009 - val_accuracy: 0.9763 - 79s/epoch - 67ms/step\n",
      "Epoch 19/40\n",
      "1188/1188 - 80s - loss: 0.1121 - accuracy: 0.9702 - val_loss: 0.0986 - val_accuracy: 0.9772 - 80s/epoch - 67ms/step\n",
      "594/594 - 5s - loss: 0.0978 - accuracy: 0.9766 - 5s/epoch - 9ms/step\n",
      "\n",
      "Fold 2 - Test Accuracy: 97.66%\n",
      "\n",
      "Training on Fold 3/5\n",
      "Model built successfully.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# ... (previous imports and code remain unchanged)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout, ELU\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "class DFNet:\n",
    "    @staticmethod\n",
    "    def build(input_shape, classes, dropout_rate1=0.7, dropout_rate2=0.5, dropout_rate_fc=0.5):\n",
    "        model = Sequential()\n",
    "\n",
    "        # ... (previous model definition remains unchanged)\n",
    "        # Block 1\n",
    "        filter_num = [None, 32, 64, 128, 256]\n",
    "        kernel_size = [None, 8, 8, 8, 8]\n",
    "        conv_stride_size = [None, 1, 1, 1, 1]\n",
    "        pool_stride_size = [None, 4, 4, 4, 4]\n",
    "        pool_size = [None, 8, 8, 8, 8]\n",
    "\n",
    "        for i in range(1, 4):\n",
    "            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n",
    "                             strides=conv_stride_size[i], padding='same',\n",
    "                             input_shape=input_shape if i == 1 else (None, input_shape[1]),\n",
    "                             name=f'block{i}_conv1'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i * 2 - 1}'))\n",
    "            model.add(ELU(alpha=1.0, name=f'block{i}_adv_act1'))\n",
    "\n",
    "            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n",
    "                             strides=conv_stride_size[i], padding='same',\n",
    "                             name=f'block{i}_conv2'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i * 2}'))\n",
    "            model.add(ELU(alpha=1.0, name=f'block{i}_adv_act2'))\n",
    "\n",
    "            model.add(MaxPooling1D(pool_size=pool_size[i], strides=pool_stride_size[i],\n",
    "                                   padding='same', name=f'block{i}_pool'))\n",
    "            model.add(Dropout(0.1, name=f'block{i}_dropout'))\n",
    "\n",
    "        # ... (rest of the model remains unchanged)\n",
    "\n",
    "        model.add(Flatten(name='flatten'))\n",
    "\n",
    "        # Fully connected layers\n",
    "        for i in range(1, 4):\n",
    "            model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name=f'fc{i}'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i + 8}'))\n",
    "            model.add(Activation('relu', name=f'fc{i}_act'))\n",
    "\n",
    "            # Experiment with different dropout rates\n",
    "            model.add(Dropout(dropout_rate_fc, name=f'fc{i}_dropout'))\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc_final'))\n",
    "        model.add(Activation('softmax', name=\"softmax\"))\n",
    "\n",
    "        print(\"Model built successfully.\")\n",
    "        return model\n",
    "\n",
    "# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\n",
    "input_shape = (5000, 1)\n",
    "num_classes = 10\n",
    "\n",
    "# Experiment with different dropout rates\n",
    "dropout_rate1 = 0.6\n",
    "dropout_rate2 = 0.4\n",
    "dropout_rate_fc = 0.5\n",
    "\n",
    "# Build the model with the specified dropout rates\n",
    "model = DFNet.build(input_shape=input_shape, classes=num_classes,\n",
    "                    dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n",
    "\n",
    "# Print the model summary\n",
    "print(\"Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.002)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\n",
    "input_shape = (5000, 1)\n",
    "num_classes = 95\n",
    "\n",
    "# Experiment with different dropout rates\n",
    "dropout_rate1 = 0.6\n",
    "dropout_rate2 = 0.4\n",
    "dropout_rate_fc = 0.5\n",
    "\n",
    "# Load and merge data using the function you defined\n",
    "X_all, y_all = LoadDataNoDefCW()\n",
    "\n",
    "# Define the number of folds\n",
    "n_splits = 5  # You can adjust this as needed\n",
    "\n",
    "# Initialize Stratified K-Fold\n",
    "stratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results\n",
    "all_histories = []\n",
    "\n",
    "# Iterate over folds\n",
    "for fold, (train_index, test_index) in enumerate(stratkf.split(X_all, y_all)):\n",
    "    print(f\"\\nTraining on Fold {fold + 1}/{n_splits}\")\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test = X_all[train_index], X_all[test_index]\n",
    "    y_train, y_test = y_all[train_index], y_all[test_index]\n",
    "\n",
    "    # Build the model\n",
    "    model = DFNet.build(input_shape=input_shape, classes=num_classes,\n",
    "                        dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.002)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    history = model.fit(X_train, y_train, epochs=40, batch_size=64, \n",
    "                        validation_data=(X_test, y_test), verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(f\"\\nFold {fold + 1} - Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Save history for later analysis if needed\n",
    "    all_histories.append(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8846fe0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T05:24:35.805418Z",
     "iopub.status.busy": "2024-01-04T05:24:35.804997Z",
     "iopub.status.idle": "2024-01-04T05:24:35.831906Z",
     "shell.execute_reply": "2024-01-04T05:24:35.830198Z",
     "shell.execute_reply.started": "2024-01-04T05:24:35.805386Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "​\n",
    "# ... (previous imports and code remain unchanged)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout, ELU\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from keras import layers, models\n",
    "import numpy as np\n",
    "​\n",
    "​\n",
    "class DFNet:\n",
    "    @staticmethod\n",
    "    def build(input_shape, classes, dropout_rate1=0.7, dropout_rate2=0.5, dropout_rate_fc=0.5):\n",
    "        model = Sequential()\n",
    "​\n",
    "        # ... (previous model definition remains unchanged)\n",
    "        # Block 1\n",
    "        filter_num = [None, 32, 64, 128, 256]\n",
    "        kernel_size = [None, 8, 8, 8, 8]\n",
    "        conv_stride_size = [None, 1, 1, 1, 1]\n",
    "        pool_stride_size = [None, 4, 4, 4, 4]\n",
    "        pool_size = [None, 8, 8, 8, 8]\n",
    "​\n",
    "        for i in range(1, 4):\n",
    "            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n",
    "                             strides=conv_stride_size[i], padding='same',\n",
    "                             input_shape=input_shape if i == 1 else (None, input_shape[1]),\n",
    "                             name=f'block{i}_conv1'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i * 2 - 1}'))\n",
    "            model.add(ELU(alpha=1.0, name=f'block{i}_adv_act1'))\n",
    "​\n",
    "            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n",
    "                             strides=conv_stride_size[i], padding='same',\n",
    "                             name=f'block{i}_conv2'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i * 2}'))\n",
    "            model.add(ELU(alpha=1.0, name=f'block{i}_adv_act2'))\n",
    "​\n",
    "            model.add(MaxPooling1D(pool_size=pool_size[i], strides=pool_stride_size[i],\n",
    "                                   padding='same', name=f'block{i}_pool'))\n",
    "            model.add(Dropout(0.1, name=f'block{i}_dropout'))\n",
    "​\n",
    "        # ... (rest of the model remains unchanged)\n",
    "​\n",
    "​\n",
    "        model.add(Flatten(name='flatten'))\n",
    "​\n",
    "​\n",
    "​\n",
    "        # Fully connected layers\n",
    "        for i in range(1, 4):\n",
    "            model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name=f'fc{i}'))\n",
    "            model.add(BatchNormalization(name=f'batch_normalization_{i + 8}'))\n",
    "            model.add(Activation('relu', name=f'fc{i}_act'))\n",
    "​\n",
    "            # Experiment with different dropout rates\n",
    "            model.add(Dropout(dropout_rate_fc, name=f'fc{i}_dropout'))\n",
    "​\n",
    "        # Output layer\n",
    "        #model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc3'))\n",
    "       # model.add(Activation('softmax', name=\"softmax\"))\n",
    "        # Output layer\n",
    "        model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc_final'))\n",
    "        model.add(Activation('softmax', name=\"softmax\"))\n",
    "​\n",
    "        print(\"Model built successfully.\")\n",
    "        return model\n",
    "​\n",
    "# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\n",
    "input_shape = (5000, 1)\n",
    "num_classes = 10\n",
    "​\n",
    "# Experiment with different dropout rates\n",
    "dropout_rate1 = 0.6\n",
    "dropout_rate2 = 0.4\n",
    "dropout_rate_fc = 0.5\n",
    "​\n",
    "# Build the model with the specified dropout rates\n",
    "model = DFNet.build(input_shape=input_shape, classes=num_classes,\n",
    "                    dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n",
    "​\n",
    "​\n",
    "# Print the model summary\n",
    "print(\"Model Summary:\")\n",
    "model.summary()\n",
    "​\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.002)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "​\n",
    "​\n",
    "# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\n",
    "input_shape = (5000, 1)\n",
    "num_classes = 95\n",
    "​\n",
    "# Experiment with different dropout rates\n",
    "dropout_rate1 = 0.6\n",
    "dropout_rate2 = 0.4\n",
    "dropout_rate_fc = 0.5\n",
    "​\n",
    "# Load and merge data using the function you defined\n",
    "X_all, y_all = LoadDataNoDefCW()\n",
    "​\n",
    "# Define the number of folds\n",
    "n_splits = 5  # You can adjust this as needed\n",
    "​\n",
    "# Initialize Stratified K-Fold\n",
    "stratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "​\n",
    "# Lists to store results\n",
    "all_histories = []\n",
    "​\n",
    "# Iterate over folds\n",
    "for fold, (train_index, test_index) in enumerate(stratkf.split(X_all, y_all)):\n",
    "    print(f\"\\nTraining on Fold {fold + 1}/{n_splits}\")\n",
    "​\n",
    "    # Split the data\n",
    "    X_train, X_test = X_all[train_index], X_all[test_index]\n",
    "    y_train, y_test = y_all[train_index], y_all[test_index]\n",
    "​\n",
    "    # Build the model\n",
    "    model = DFNet.build(input_shape=input_shape, classes=num_classes,\n",
    "                        dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n",
    "​\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.002)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "​\n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "​\n",
    "    # Train the model with early stopping\n",
    "    history = model.fit(X_train, y_train, epochs=40, batch_size=64, \n",
    "                        validation_data=(X_test, y_test), verbose=2, callbacks=[early_stopping])\n",
    "​\n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(f\"\\nFold {fold + 1} - Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "​\n",
    "    # Save history for later analysis if needed\n",
    "    all_histories.append(history)\n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e17dc6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4227018,
     "sourceId": 7288815,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 37157.036905,
   "end_time": "2024-01-06T21:36:34.926637",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-06T11:17:17.889732",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
